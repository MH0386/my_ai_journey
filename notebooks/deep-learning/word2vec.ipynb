{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.kaggle.com/code/mh0386/word2vec?scriptVersionId=249005362\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts1 = \"Word2vec is a technique for natural language processing (NLP) published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that they capture the semantic and syntactic qualities of words; as such, a simple mathematical function (cosine similarity) can indicate the level of semantic similarity between the words represented by those vectors. Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word2vec can utilize either of two model architectures to produce these distributed representations of words: continuously sliding bag-of-words (CBOW) or continuously sliding skip-gram. In both architectures, word2vec considers both individual words and a sliding context window as it iterates over the corpus. The CBOW can be viewed as a ‘fill in the blank’ task, where the word embedding represents the way the word influences the relative probabilities of other words in the context window. Words which are semantically similar should influence these probabilities in similar ways, because semantically similar words should be used in similar contexts. The order of context words does not influence prediction (bag-of-words assumption). In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words.[1][2] The skip-gram architecture weighs nearby context words more heavily than more distant context words. According to the authors' note,[3] CBOW is faster while skip-gram does a better job for infrequent words. After the model has trained, the learned word embeddings are positioned in the vector space such that words that share common contexts in the corpus — that is, words that are semantically and syntactically similar — are located close to one another in the space.[1] More dissimilar words are located farther from one another in the space\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts2 = \"GloVe, coined from Global Vectors, is a model for distributed word representation. The model is an unsupervised learning algorithm for obtaining vector representations for words. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity.[1] Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. It is developed as an open-source project at Stanford[2] and was launched in 2014. As log-bilinear regression model for unsupervised learning of word representations, it combines the features of two model families, namely the global matrix factorization and local context window methods.[3] GloVe can be used to find relations between words like synonyms, company-product relations, zip codes and cities, etc. However, the unsupervised learning algorithm is not effective in identifying homographs, that is, words with the same spelling and different meanings. This is as the unsupervised learning algorithm calculates a single set of vectors for words with the same morphological structure.[4] The algorithm is also used by the SpaCy library to build semantic word embedding features, while computing the top list words that match with distance measures such as cosine similarity and Euclidean distance approach.[5] GloVe was also used as the word representation framework for the online and offline systems designed to detect psychological distress in patient interviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts1 = texts1.split(\".\")\n",
    "texts2 = texts2.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"len of text1: {len(texts1)}, len of text2: {len(texts2)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(\n",
    "    TEXT: str, punctuations=r\"\"\"!()-[]{};:'\"\\,<>./?@#$%^&*_“~\"\"\", stop_words=None\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    A method to preprocess a text\n",
    "    \"\"\"\n",
    "    if stop_words is None:\n",
    "        stop_words = [\n",
    "            \"and\",\n",
    "            \"a\",\n",
    "            \"is\",\n",
    "            \"the\",\n",
    "            \"in\",\n",
    "            \"be\",\n",
    "            \"will\",\n",
    "            \"was\",\n",
    "            \"but\",\n",
    "            \"this\",\n",
    "            \"were\",\n",
    "            \"with\",\n",
    "            \"of\",\n",
    "            \"also\",\n",
    "            \"on\",\n",
    "            \".\",\n",
    "            \"for\",\n",
    "            \"any\",\n",
    "            \"its\",\n",
    "            \"and\",\n",
    "            \"are\",\n",
    "            \"from\",\n",
    "            \"both\",\n",
    "            \"as\",\n",
    "            \"to\",\n",
    "            \"these\",\n",
    "            \"—\",\n",
    "            \"‘\",\n",
    "            \"can\",\n",
    "            \"does\",\n",
    "            \"other\",\n",
    "            \"because\",\n",
    "            \"over\",\n",
    "            \"it\",\n",
    "            \"where\",\n",
    "        ]\n",
    "\n",
    "    for x in TEXT.lower():\n",
    "        if x in punctuations:\n",
    "            TEXT = TEXT.replace(x, \"\")\n",
    "\n",
    "    # Removing words that have numbers in them\n",
    "    TEXT = re.sub(r\"\\w*\\d\\w*\", \"\", TEXT)\n",
    "\n",
    "    # Removing digits\n",
    "    TEXT = re.sub(r\"[0-9]+\", \"\", TEXT)\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    TEXT = re.sub(r\"\\s+\", \" \", TEXT).strip()\n",
    "\n",
    "    # Setting every word to lower\n",
    "    TEXT = TEXT.lower()\n",
    "\n",
    "    # Converting all our text to a list\n",
    "    TEXT = TEXT.split(\" \")\n",
    "\n",
    "    # Dropping empty strings\n",
    "    TEXT = [x for x in TEXT if x != \"\"]\n",
    "\n",
    "    # Dropping stop words\n",
    "    TEXT = [x for x in TEXT if x not in stop_words]\n",
    "\n",
    "    return TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(texts: list, window=2):\n",
    "    # Defining the window for context\n",
    "    # window = 2\n",
    "\n",
    "    # Creating a placeholder for the scanning of the word list\n",
    "    word_lists = []\n",
    "    all_text = []\n",
    "    for text in texts:\n",
    "        # Cleaning the text\n",
    "        text = text_preprocessing(text)\n",
    "        print(text)\n",
    "\n",
    "        # Appending to the all text lists\n",
    "        all_text += text\n",
    "\n",
    "        # Creating a context dictionary\n",
    "        for i, word in enumerate(text):\n",
    "            for w in range(window):\n",
    "                # Getting the context that is ahead by *window* words\n",
    "                if i + 1 + w < len(text):\n",
    "                    word_lists.append([word] + [text[(i + 1 + w)]])\n",
    "                # Getting the context that is behind by *window* words\n",
    "                if i - w - 1 >= 0:\n",
    "                    word_lists.append([word] + [text[(i - w - 1)]])\n",
    "    return word_lists, list(set(all_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lists1, all_text1 = get_training_data(texts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lists1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unique_word_dict(TEXT: list):\n",
    "    \"\"\"\n",
    "    A method that creates a dictionary where the keys are unique words\n",
    "    and key values are indices\n",
    "    \"\"\"\n",
    "    # Getting all the unique words from our text and sorting them alphabetically\n",
    "    Words = list(set(TEXT))\n",
    "    Words.sort()\n",
    "\n",
    "    # Creating the dictionary for the unique words\n",
    "    unique_word_dict = {}\n",
    "    for i, Word in enumerate(Words):\n",
    "        unique_word_dict.update({Word: i})\n",
    "\n",
    "    return unique_word_dict, len(unique_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word_dict1, number_of_unique_words1 = create_unique_word_dict(all_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word_dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_unique_words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the unique words\n",
    "unique_word1 = list(unique_word_dict1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_representation(word_lists, number_of_unique_words, unique_word_dict):\n",
    "    # Creating the X and Y matrices using one hot encoding\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i, word_list in tqdm(enumerate(word_lists)):\n",
    "        # Getting the indices\n",
    "        main_word_index = unique_word_dict.get(word_list[0])\n",
    "        context_word_index = unique_word_dict.get(word_list[1])\n",
    "\n",
    "        print(word_list)\n",
    "        print(word_list[0], main_word_index)\n",
    "        print(word_list[1], context_word_index)\n",
    "\n",
    "        # Creating the placeholders\n",
    "        X_row = np.zeros(number_of_unique_words)\n",
    "        Y_row = np.zeros(number_of_unique_words)\n",
    "\n",
    "        # One hot encoding the main word\n",
    "        X_row[main_word_index] = 1\n",
    "\n",
    "        # One hot encoding the Y matrix words\n",
    "        Y_row[context_word_index] = 1\n",
    "\n",
    "        # Appending to the main matrices\n",
    "        X.append(X_row)\n",
    "        Y.append(Y_row)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_x1, matrix_y1 = one_hot_representation(\n",
    "    word_lists1, number_of_unique_words1, unique_word_dict1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_x1 = tf.convert_to_tensor(matrix_x1, dtype=tf.float32)\n",
    "matrix_y1 = tf.convert_to_tensor(matrix_y1, dtype=tf.float32)\n",
    "print(matrix_x1.shape)\n",
    "print(matrix_y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateModel(input, output):\n",
    "    # Defining the size of the embedding\n",
    "    embed_size = 2\n",
    "    # Defining the neural network\n",
    "\n",
    "    inp = Input(shape=input)  # 21\n",
    "    x = Dense(units=embed_size, activation=\"linear\")(inp)\n",
    "    x = Dense(units=output, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=\"accuracy\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = CreateModel(matrix_x1.shape[1], matrix_y1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(x=matrix_x1, y=matrix_y1, epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(model1.history.history[\"loss\"])\n",
    "plt.plot(model1.history.history[\"accuracy\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input layer\n",
    "\n",
    "weights1 = model1.get_weights()[0]\n",
    "print(weights1.shape)\n",
    "# print(weights[1][1])\n",
    "# print(weights)\n",
    "\n",
    "# weights = model.get_weights()[2]\n",
    "# print (weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_word_dict(unique_word, unique_word_dict, weights):\n",
    "    # get the weight for each unique word\n",
    "    embedding_dict = {}\n",
    "    for word in unique_word:  # to pick a row of weight of two values for each unique word since weights = 21*2\n",
    "        embedding_dict.update({word: weights[unique_word_dict.get(word)]})\n",
    "    return embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict1 = create_embedding_word_dict(unique_word1, unique_word_dict1, weights1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# fig, axs = plt.subplots(len(embedding_dict.keys()), 3, figsize=(15, 10), subplot_kw={'projection': '3d'})\n",
    "# print(len(embedding_dict.keys()))\n",
    "# axs = axs.flatten()\n",
    "# # Iterate through unique_word and plot in each subplot\n",
    "# for i, word in enumerate(unique_word):\n",
    "#     coord = embedding_dict.get(word)\n",
    "#     ax = axs[i]\n",
    "\n",
    "#     if coord is not None:\n",
    "#         ax.scatter(coord[0], coord[1], coord[2], c='b', marker='o')\n",
    "# ax.text(coord[0], coord[1], coord[2], s=word)\n",
    "# ax.set_xlabel('X')\n",
    "# ax.set_ylabel('Y')\n",
    "# ax.set_zlabel('Z')\n",
    "# ax.set_title(word)\n",
    "\n",
    "# Remove empty subplots\n",
    "# for j in range(len(unique_word), len(axs)):\n",
    "#     fig.delaxes(axs[j])\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# i = 0\n",
    "# ax_list = []\n",
    "# for word in unique_word:\n",
    "#     #print(i, ' >> ', word)\n",
    "#     coord = embedding_dict.get(word)\n",
    "#     fig = plt.figure()\n",
    "#     ax = fig.add_subplot( projection='3d')\n",
    "#     ax.scatter(coord[0], coord[1], coord[2], c='b', marker='o')\n",
    "#     ax.text(coord[0], coord[1], coord[2], s=word)\n",
    "#     ax.set_xlabel('X')\n",
    "#     ax.set_ylabel('Y')\n",
    "#     ax.set_zlabel('Z')\n",
    "#     ax.legend()\n",
    "#     ax_list.append(ax)\n",
    "\n",
    "#     # plt.scatter(coord[0], coord[1])\n",
    "#     # plt.annotate(word, (coord[0], coord[1]))\n",
    "#     i = i + 1\n",
    "#     plt.legend(unique_word)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for word in unique_word1:\n",
    "    coord = embedding_dict1.get(word)\n",
    "    plt.scatter(coord[0], coord[1])\n",
    "    plt.annotate(word, (coord[0], coord[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "i = 0\n",
    "weights1 = model1.get_weights()[0]\n",
    "for word in list(unique_word_dict1.keys()):\n",
    "    coord = embedding_dict1.get(word)\n",
    "    if weights1[i][0] < 0 < weights1[i][1]:\n",
    "        plt.scatter(0, weights1[i][1])\n",
    "        plt.annotate(word, (0, weights1[i][1]))\n",
    "    else:\n",
    "        plt.scatter(weights1[i][0], weights1[i][1])\n",
    "        plt.annotate(word, (weights1[i][0], weights1[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lists2, all_text2 = get_training_data(texts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lists2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word_dict2, number_of_unique_words2 = create_unique_word_dict(all_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_unique_words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word2 = list(unique_word_dict2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_x2, matrix_y2 = one_hot_representation(\n",
    "    word_lists2, number_of_unique_words2, unique_word_dict2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_x2 = tf.convert_to_tensor(matrix_x2, dtype=tf.float32)\n",
    "matrix_y2 = tf.convert_to_tensor(matrix_y2, dtype=tf.float32)\n",
    "print(matrix_x2.shape)\n",
    "print(matrix_y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = CreateModel(matrix_x2.shape[1], matrix_y2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(x=matrix_x2, y=matrix_y2, epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(model2.history.history[\"loss\"])\n",
    "plt.plot(model2.history.history[\"accuracy\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights2 = model2.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict2 = create_embedding_word_dict(unique_word2, unique_word_dict2, weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for word in unique_word2:\n",
    "    coord = embedding_dict2.get(word)\n",
    "    plt.scatter(coord[0], coord[1])\n",
    "    plt.annotate(word, (coord[0], coord[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "i = 0\n",
    "for word in list(unique_word_dict2.keys()):\n",
    "    coord = embedding_dict2.get(word)\n",
    "    if weights2[i][0] < 0 < weights2[i][1]:\n",
    "        plt.scatter(0, weights2[i][1])\n",
    "        plt.annotate(word, (0, weights2[i][1]))\n",
    "    else:\n",
    "        plt.scatter(weights2[i][0], weights2[i][1])\n",
    "        plt.annotate(word, (weights2[i][0], weights2[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = list(embedding_dict1.values())\n",
    "embedding2 = list(embedding_dict2.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosineSimilarity = cosine_similarity(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosineSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cosineSimilarity, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclideanDistances = euclidean_distances(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclideanDistances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(euclideanDistances, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = np.array(embedding1)\n",
    "embedding2 = np.array(embedding2)\n",
    "embedding = np.concatenate((embedding1, embedding2), axis=0)\n",
    "embedding_s = np.cov(embedding)\n",
    "embedding_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "pos = mds.fit_transform(euclideanDistance)\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for x, y, name in zip(\n",
    "    xs, ys, list(unique_word_dict1.keys()) + list(unique_word_dict2.keys())\n",
    "):\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(name, (x, y))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 81.736376,
   "end_time": "2023-12-01T13:38:45.297083",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-01T13:37:23.560707",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
