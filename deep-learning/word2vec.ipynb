{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  },
  "papermill": {
   "default_parameters": {},
   "duration": 81.736376,
   "end_time": "2023-12-01T13:38:45.297083",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-01T13:37:23.560707",
   "version": "2.4.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "source": "<a href=\"https://www.kaggle.com/code/mh0386/word2vec?scriptVersionId=249005362\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>",
   "metadata": {},
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport re\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\nimport seaborn as sns",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 16.227106,
     "end_time": "2023-12-01T13:37:43.39827",
     "exception": false,
     "start_time": "2023-12-01T13:37:27.171164",
     "status": "completed"
    },
    "tags": [],
    "id": "77a8361c",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:26:59.341891Z",
     "start_time": "2023-12-03T05:26:48.925720300Z"
    },
    "editable": false,
    "_kg_hide-output": true,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:22.850226Z",
     "iopub.execute_input": "2025-07-05T17:37:22.850477Z",
     "iopub.status.idle": "2025-07-05T17:37:36.927376Z",
     "shell.execute_reply.started": "2025-07-05T17:37:22.850459Z",
     "shell.execute_reply": "2025-07-05T17:37:36.926842Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "2025-07-05 17:37:24.381879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751737044.591053      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751737044.649010      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "texts1 = \"Word2vec is a technique for natural language processing (NLP) published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that they capture the semantic and syntactic qualities of words; as such, a simple mathematical function (cosine similarity) can indicate the level of semantic similarity between the words represented by those vectors. Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word2vec can utilize either of two model architectures to produce these distributed representations of words: continuously sliding bag-of-words (CBOW) or continuously sliding skip-gram. In both architectures, word2vec considers both individual words and a sliding context window as it iterates over the corpus. The CBOW can be viewed as a ‘fill in the blank’ task, where the word embedding represents the way the word influences the relative probabilities of other words in the context window. Words which are semantically similar should influence these probabilities in similar ways, because semantically similar words should be used in similar contexts. The order of context words does not influence prediction (bag-of-words assumption). In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words.[1][2] The skip-gram architecture weighs nearby context words more heavily than more distant context words. According to the authors' note,[3] CBOW is faster while skip-gram does a better job for infrequent words. After the model has trained, the learned word embeddings are positioned in the vector space such that words that share common contexts in the corpus — that is, words that are semantically and syntactically similar — are located close to one another in the space.[1] More dissimilar words are located farther from one another in the space\"",
   "metadata": {
    "papermill": {
     "duration": 0.029585,
     "end_time": "2023-12-01T13:37:43.435548",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.405963",
     "status": "completed"
    },
    "tags": [],
    "id": "d7df27f1",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:26:59.389057500Z",
     "start_time": "2023-12-03T05:26:59.344893200Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:36.928489Z",
     "iopub.execute_input": "2025-07-05T17:37:36.929188Z",
     "iopub.status.idle": "2025-07-05T17:37:36.933949Z",
     "shell.execute_reply.started": "2025-07-05T17:37:36.929168Z",
     "shell.execute_reply": "2025-07-05T17:37:36.933232Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "texts2 = \"GloVe, coined from Global Vectors, is a model for distributed word representation. The model is an unsupervised learning algorithm for obtaining vector representations for words. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity.[1] Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. It is developed as an open-source project at Stanford[2] and was launched in 2014. As log-bilinear regression model for unsupervised learning of word representations, it combines the features of two model families, namely the global matrix factorization and local context window methods.[3] GloVe can be used to find relations between words like synonyms, company-product relations, zip codes and cities, etc. However, the unsupervised learning algorithm is not effective in identifying homographs, that is, words with the same spelling and different meanings. This is as the unsupervised learning algorithm calculates a single set of vectors for words with the same morphological structure.[4] The algorithm is also used by the SpaCy library to build semantic word embedding features, while computing the top list words that match with distance measures such as cosine similarity and Euclidean distance approach.[5] GloVe was also used as the word representation framework for the online and offline systems designed to detect psychological distress in patient interviews\""
   ],
   "metadata": {
    "id": "e62694782658baee",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:26:59.465607700Z",
     "start_time": "2023-12-03T05:26:59.354248600Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:36.934603Z",
     "iopub.execute_input": "2025-07-05T17:37:36.935169Z",
     "iopub.status.idle": "2025-07-05T17:37:36.988302Z",
     "shell.execute_reply.started": "2025-07-05T17:37:36.935143Z",
     "shell.execute_reply": "2025-07-05T17:37:36.987623Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "texts1 = texts1.split(\".\")\n",
    "texts2 = texts2.split(\".\")"
   ],
   "metadata": {
    "id": "6c89d20b47662a7a",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:26:59.466607800Z",
     "start_time": "2023-12-03T05:26:59.373269500Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:36.989915Z",
     "iopub.execute_input": "2025-07-05T17:37:36.990087Z",
     "iopub.status.idle": "2025-07-05T17:37:37.006244Z",
     "shell.execute_reply.started": "2025-07-05T17:37:36.990073Z",
     "shell.execute_reply": "2025-07-05T17:37:37.005686Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "f\"len of text1: {len(texts1)}, len of text2: {len(texts2)}\""
   ],
   "metadata": {
    "papermill": {
     "duration": 0.019802,
     "end_time": "2023-12-01T13:37:43.494716",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.474914",
     "status": "completed"
    },
    "tags": [],
    "id": "04392171",
    "outputId": "c56a059c-c410-4aec-9334-5130c879948e",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:26:59.525864300Z",
     "start_time": "2023-12-03T05:26:59.383058700Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:37.007045Z",
     "iopub.execute_input": "2025-07-05T17:37:37.007237Z",
     "iopub.status.idle": "2025-07-05T17:37:37.021888Z",
     "shell.execute_reply.started": "2025-07-05T17:37:37.007223Z",
     "shell.execute_reply": "2025-07-05T17:37:37.021335Z"
    }
   },
   "outputs": [
    {
     "execution_count": 5,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'len of text1: 18, len of text2: 11'"
     },
     "metadata": {}
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "texts1",
   "metadata": {
    "papermill": {
     "duration": 0.019024,
     "end_time": "2023-12-01T13:37:43.521789",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.502765",
     "status": "completed"
    },
    "tags": [],
    "id": "5cf74f7e",
    "outputId": "3ede7f26-ca04-4f36-cd4a-ca76ee637e6b",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:26:59.527865900Z",
     "start_time": "2023-12-03T05:26:59.399542Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:37.022535Z",
     "iopub.execute_input": "2025-07-05T17:37:37.022773Z",
     "iopub.status.idle": "2025-07-05T17:37:37.037069Z",
     "shell.execute_reply.started": "2025-07-05T17:37:37.022758Z",
     "shell.execute_reply": "2025-07-05T17:37:37.036395Z"
    }
   },
   "outputs": [
    {
     "execution_count": 6,
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Word2vec is a technique for natural language processing (NLP) published in 2013',\n ' The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text',\n ' Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence',\n ' As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector',\n ' The vectors are chosen carefully such that they capture the semantic and syntactic qualities of words; as such, a simple mathematical function (cosine similarity) can indicate the level of semantic similarity between the words represented by those vectors',\n ' Word2vec is a group of related models that are used to produce word embeddings',\n ' These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words',\n ' Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space',\n ' Word2vec can utilize either of two model architectures to produce these distributed representations of words: continuously sliding bag-of-words (CBOW) or continuously sliding skip-gram',\n ' In both architectures, word2vec considers both individual words and a sliding context window as it iterates over the corpus',\n ' The CBOW can be viewed as a ‘fill in the blank’ task, where the word embedding represents the way the word influences the relative probabilities of other words in the context window',\n ' Words which are semantically similar should influence these probabilities in similar ways, because semantically similar words should be used in similar contexts',\n ' The order of context words does not influence prediction (bag-of-words assumption)',\n ' In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words',\n '[1][2] The skip-gram architecture weighs nearby context words more heavily than more distant context words',\n \" According to the authors' note,[3] CBOW is faster while skip-gram does a better job for infrequent words\",\n ' After the model has trained, the learned word embeddings are positioned in the vector space such that words that share common contexts in the corpus — that is, words that are semantically and syntactically similar — are located close to one another in the space',\n '[1] More dissimilar words are located farther from one another in the space']"
     },
     "metadata": {}
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "texts2",
   "metadata": {
    "id": "488abb9851ea9b5b",
    "outputId": "a3867071-e6d3-4ba7-d9da-c070a3765689",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:26:59.540864100Z",
     "start_time": "2023-12-03T05:26:59.412462Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:37.037771Z",
     "iopub.execute_input": "2025-07-05T17:37:37.037953Z",
     "iopub.status.idle": "2025-07-05T17:37:37.051243Z",
     "shell.execute_reply.started": "2025-07-05T17:37:37.03794Z",
     "shell.execute_reply": "2025-07-05T17:37:37.050699Z"
    }
   },
   "outputs": [
    {
     "execution_count": 7,
     "output_type": "execute_result",
     "data": {
      "text/plain": "['GloVe, coined from Global Vectors, is a model for distributed word representation',\n ' The model is an unsupervised learning algorithm for obtaining vector representations for words',\n ' This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity',\n '[1] Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space',\n ' It is developed as an open-source project at Stanford[2] and was launched in 2014',\n ' As log-bilinear regression model for unsupervised learning of word representations, it combines the features of two model families, namely the global matrix factorization and local context window methods',\n '[3] GloVe can be used to find relations between words like synonyms, company-product relations, zip codes and cities, etc',\n ' However, the unsupervised learning algorithm is not effective in identifying homographs, that is, words with the same spelling and different meanings',\n ' This is as the unsupervised learning algorithm calculates a single set of vectors for words with the same morphological structure',\n '[4] The algorithm is also used by the SpaCy library to build semantic word embedding features, while computing the top list words that match with distance measures such as cosine similarity and Euclidean distance approach',\n '[5] GloVe was also used as the word representation framework for the online and offline systems designed to detect psychological distress in patient interviews']"
     },
     "metadata": {}
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "def text_preprocessing(\n",
    "    TEXT: str, punctuations=r\"\"\"!()-[]{};:'\"\\,<>./?@#$%^&*_“~\"\"\", stop_words=None\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    A method to preprocess a text\n",
    "    \"\"\"\n",
    "    if stop_words is None:\n",
    "        stop_words = [\n",
    "            \"and\",\n",
    "            \"a\",\n",
    "            \"is\",\n",
    "            \"the\",\n",
    "            \"in\",\n",
    "            \"be\",\n",
    "            \"will\",\n",
    "            \"was\",\n",
    "            \"but\",\n",
    "            \"this\",\n",
    "            \"were\",\n",
    "            \"with\",\n",
    "            \"of\",\n",
    "            \"also\",\n",
    "            \"on\",\n",
    "            \".\",\n",
    "            \"for\",\n",
    "            \"any\",\n",
    "            \"its\",\n",
    "            \"and\",\n",
    "            \"are\",\n",
    "            \"from\",\n",
    "            \"both\",\n",
    "            \"as\",\n",
    "            \"to\",\n",
    "            \"these\",\n",
    "            \"—\",\n",
    "            \"‘\",\n",
    "            \"can\",\n",
    "            \"does\",\n",
    "            \"other\",\n",
    "            \"because\",\n",
    "            \"over\",\n",
    "            \"it\",\n",
    "            \"where\",\n",
    "        ]\n",
    "\n",
    "    for x in TEXT.lower():\n",
    "        if x in punctuations:\n",
    "            TEXT = TEXT.replace(x, \"\")\n",
    "\n",
    "    # Removing words that have numbers in them\n",
    "    TEXT = re.sub(r\"\\w*\\d\\w*\", \"\", TEXT)\n",
    "\n",
    "    # Removing digits\n",
    "    TEXT = re.sub(r\"[0-9]+\", \"\", TEXT)\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    TEXT = re.sub(r\"\\s+\", \" \", TEXT).strip()\n",
    "\n",
    "    # Setting every word to lower\n",
    "    TEXT = TEXT.lower()\n",
    "\n",
    "    # Converting all our text to a list\n",
    "    TEXT = TEXT.split(\" \")\n",
    "\n",
    "    # Dropping empty strings\n",
    "    TEXT = [x for x in TEXT if x != \"\"]\n",
    "\n",
    "    # Dropping stop words\n",
    "    TEXT = [x for x in TEXT if x not in stop_words]\n",
    "\n",
    "    return TEXT"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.022867,
     "end_time": "2023-12-01T13:37:43.552555",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.529688",
     "status": "completed"
    },
    "tags": [],
    "id": "c7ebfaab",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:26:59.541863900Z",
     "start_time": "2023-12-03T05:26:59.426540900Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:37.051894Z",
     "iopub.execute_input": "2025-07-05T17:37:37.052157Z",
     "iopub.status.idle": "2025-07-05T17:37:37.064843Z",
     "shell.execute_reply.started": "2025-07-05T17:37:37.052141Z",
     "shell.execute_reply": "2025-07-05T17:37:37.064188Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "def get_training_data(texts: list, window=2):\n    # Defining the window for context\n    # window = 2\n\n    # Creating a placeholder for the scanning of the word list\n    word_lists = []\n    all_text = []\n    for text in texts:\n        # Cleaning the text\n        text = text_preprocessing(text)\n        print(text)\n\n        # Appending to the all text lists\n        all_text += text\n\n        # Creating a context dictionary\n        for i, word in enumerate(text):\n            for w in range(window):\n                # Getting the context that is ahead by *window* words\n                if i + 1 + w < len(text):\n                    word_lists.append([word] + [text[(i + 1 + w)]])\n                # Getting the context that is behind by *window* words\n                if i - w - 1 >= 0:\n                    word_lists.append([word] + [text[(i - w - 1)]])\n    return word_lists, list(set(all_text))",
   "metadata": {
    "papermill": {
     "duration": 0.020683,
     "end_time": "2023-12-01T13:37:43.581368",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.560685",
     "status": "completed"
    },
    "tags": [],
    "id": "f1fff23c",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:26:59.542865900Z",
     "start_time": "2023-12-03T05:26:59.444544700Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:37.065496Z",
     "iopub.execute_input": "2025-07-05T17:37:37.065817Z",
     "iopub.status.idle": "2025-07-05T17:37:37.082302Z",
     "shell.execute_reply.started": "2025-07-05T17:37:37.065796Z",
     "shell.execute_reply": "2025-07-05T17:37:37.08171Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "word_lists1, all_text1 = get_training_data(texts1)",
   "metadata": {
    "papermill": {
     "duration": 0.025201,
     "end_time": "2023-12-01T13:37:43.614636",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.589435",
     "status": "completed"
    },
    "tags": [],
    "id": "a30fed9a",
    "outputId": "82da4689-ac66-455c-9d24-801085a980ee",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:26:59.547416700Z",
     "start_time": "2023-12-03T05:26:59.452561200Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:37.084674Z",
     "iopub.execute_input": "2025-07-05T17:37:37.085195Z",
     "iopub.status.idle": "2025-07-05T17:37:37.102697Z",
     "shell.execute_reply.started": "2025-07-05T17:37:37.08518Z",
     "shell.execute_reply": "2025-07-05T17:37:37.102155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "['technique', 'natural', 'language', 'processing', 'nlp', 'published']\n['algorithm', 'uses', 'neural', 'network', 'model', 'learn', 'word', 'associations', 'large', 'corpus', 'text']\n['once', 'trained', 'such', 'model', 'detect', 'synonymous', 'words', 'or', 'suggest', 'additional', 'words', 'partial', 'sentence']\n['name', 'implies', 'represents', 'each', 'distinct', 'word', 'particular', 'list', 'numbers', 'called', 'vector']\n['vectors', 'chosen', 'carefully', 'such', 'that', 'they', 'capture', 'semantic', 'syntactic', 'qualities', 'words', 'such', 'simple', 'mathematical', 'function', 'cosine', 'similarity', 'indicate', 'level', 'semantic', 'similarity', 'between', 'words', 'represented', 'by', 'those', 'vectors']\n['group', 'related', 'models', 'that', 'used', 'produce', 'word', 'embeddings']\n['models', 'shallow', 'twolayer', 'neural', 'networks', 'that', 'trained', 'reconstruct', 'linguistic', 'contexts', 'words']\n['takes', 'input', 'large', 'corpus', 'text', 'produces', 'vector', 'space', 'typically', 'several', 'hundred', 'dimensions', 'each', 'unique', 'word', 'corpus', 'being', 'assigned', 'corresponding', 'vector', 'space']\n['utilize', 'either', 'two', 'model', 'architectures', 'produce', 'distributed', 'representations', 'words', 'continuously', 'sliding', 'bagofwords', 'cbow', 'or', 'continuously', 'sliding', 'skipgram']\n['architectures', 'considers', 'individual', 'words', 'sliding', 'context', 'window', 'iterates', 'corpus']\n['cbow', 'viewed', '‘fill', 'blank’', 'task', 'word', 'embedding', 'represents', 'way', 'word', 'influences', 'relative', 'probabilities', 'words', 'context', 'window']\n['words', 'which', 'semantically', 'similar', 'should', 'influence', 'probabilities', 'similar', 'ways', 'semantically', 'similar', 'words', 'should', 'used', 'similar', 'contexts']\n['order', 'context', 'words', 'not', 'influence', 'prediction', 'bagofwords', 'assumption']\n['continuous', 'skipgram', 'architecture', 'model', 'uses', 'current', 'word', 'predict', 'surrounding', 'window', 'context', 'words']\n['skipgram', 'architecture', 'weighs', 'nearby', 'context', 'words', 'more', 'heavily', 'than', 'more', 'distant', 'context', 'words']\n['according', 'authors', 'cbow', 'faster', 'while', 'skipgram', 'better', 'job', 'infrequent', 'words']\n['after', 'model', 'has', 'trained', 'learned', 'word', 'embeddings', 'positioned', 'vector', 'space', 'such', 'that', 'words', 'that', 'share', 'common', 'contexts', 'corpus', 'that', 'words', 'that', 'semantically', 'syntactically', 'similar', 'located', 'close', 'one', 'another', 'space']\n['more', 'dissimilar', 'words', 'located', 'farther', 'one', 'another', 'space']\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "word_lists1",
   "metadata": {
    "id": "743ae64ff64a6e30",
    "outputId": "4dbdc705-a2aa-499a-f3d6-a07c003111d3",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:26:59.910394300Z",
     "start_time": "2023-12-03T05:26:59.467605800Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:37.103383Z",
     "iopub.execute_input": "2025-07-05T17:37:37.103703Z",
     "iopub.status.idle": "2025-07-05T17:37:37.138474Z",
     "shell.execute_reply.started": "2025-07-05T17:37:37.103681Z",
     "shell.execute_reply": "2025-07-05T17:37:37.137845Z"
    }
   },
   "outputs": [
    {
     "execution_count": 11,
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['technique', 'natural'],\n ['technique', 'language'],\n ['natural', 'language'],\n ['natural', 'technique'],\n ['natural', 'processing'],\n ['language', 'processing'],\n ['language', 'natural'],\n ['language', 'nlp'],\n ['language', 'technique'],\n ['processing', 'nlp'],\n ['processing', 'language'],\n ['processing', 'published'],\n ['processing', 'natural'],\n ['nlp', 'published'],\n ['nlp', 'processing'],\n ['nlp', 'language'],\n ['published', 'nlp'],\n ['published', 'processing'],\n ['algorithm', 'uses'],\n ['algorithm', 'neural'],\n ['uses', 'neural'],\n ['uses', 'algorithm'],\n ['uses', 'network'],\n ['neural', 'network'],\n ['neural', 'uses'],\n ['neural', 'model'],\n ['neural', 'algorithm'],\n ['network', 'model'],\n ['network', 'neural'],\n ['network', 'learn'],\n ['network', 'uses'],\n ['model', 'learn'],\n ['model', 'network'],\n ['model', 'word'],\n ['model', 'neural'],\n ['learn', 'word'],\n ['learn', 'model'],\n ['learn', 'associations'],\n ['learn', 'network'],\n ['word', 'associations'],\n ['word', 'learn'],\n ['word', 'large'],\n ['word', 'model'],\n ['associations', 'large'],\n ['associations', 'word'],\n ['associations', 'corpus'],\n ['associations', 'learn'],\n ['large', 'corpus'],\n ['large', 'associations'],\n ['large', 'text'],\n ['large', 'word'],\n ['corpus', 'text'],\n ['corpus', 'large'],\n ['corpus', 'associations'],\n ['text', 'corpus'],\n ['text', 'large'],\n ['once', 'trained'],\n ['once', 'such'],\n ['trained', 'such'],\n ['trained', 'once'],\n ['trained', 'model'],\n ['such', 'model'],\n ['such', 'trained'],\n ['such', 'detect'],\n ['such', 'once'],\n ['model', 'detect'],\n ['model', 'such'],\n ['model', 'synonymous'],\n ['model', 'trained'],\n ['detect', 'synonymous'],\n ['detect', 'model'],\n ['detect', 'words'],\n ['detect', 'such'],\n ['synonymous', 'words'],\n ['synonymous', 'detect'],\n ['synonymous', 'or'],\n ['synonymous', 'model'],\n ['words', 'or'],\n ['words', 'synonymous'],\n ['words', 'suggest'],\n ['words', 'detect'],\n ['or', 'suggest'],\n ['or', 'words'],\n ['or', 'additional'],\n ['or', 'synonymous'],\n ['suggest', 'additional'],\n ['suggest', 'or'],\n ['suggest', 'words'],\n ['suggest', 'words'],\n ['additional', 'words'],\n ['additional', 'suggest'],\n ['additional', 'partial'],\n ['additional', 'or'],\n ['words', 'partial'],\n ['words', 'additional'],\n ['words', 'sentence'],\n ['words', 'suggest'],\n ['partial', 'sentence'],\n ['partial', 'words'],\n ['partial', 'additional'],\n ['sentence', 'partial'],\n ['sentence', 'words'],\n ['name', 'implies'],\n ['name', 'represents'],\n ['implies', 'represents'],\n ['implies', 'name'],\n ['implies', 'each'],\n ['represents', 'each'],\n ['represents', 'implies'],\n ['represents', 'distinct'],\n ['represents', 'name'],\n ['each', 'distinct'],\n ['each', 'represents'],\n ['each', 'word'],\n ['each', 'implies'],\n ['distinct', 'word'],\n ['distinct', 'each'],\n ['distinct', 'particular'],\n ['distinct', 'represents'],\n ['word', 'particular'],\n ['word', 'distinct'],\n ['word', 'list'],\n ['word', 'each'],\n ['particular', 'list'],\n ['particular', 'word'],\n ['particular', 'numbers'],\n ['particular', 'distinct'],\n ['list', 'numbers'],\n ['list', 'particular'],\n ['list', 'called'],\n ['list', 'word'],\n ['numbers', 'called'],\n ['numbers', 'list'],\n ['numbers', 'vector'],\n ['numbers', 'particular'],\n ['called', 'vector'],\n ['called', 'numbers'],\n ['called', 'list'],\n ['vector', 'called'],\n ['vector', 'numbers'],\n ['vectors', 'chosen'],\n ['vectors', 'carefully'],\n ['chosen', 'carefully'],\n ['chosen', 'vectors'],\n ['chosen', 'such'],\n ['carefully', 'such'],\n ['carefully', 'chosen'],\n ['carefully', 'that'],\n ['carefully', 'vectors'],\n ['such', 'that'],\n ['such', 'carefully'],\n ['such', 'they'],\n ['such', 'chosen'],\n ['that', 'they'],\n ['that', 'such'],\n ['that', 'capture'],\n ['that', 'carefully'],\n ['they', 'capture'],\n ['they', 'that'],\n ['they', 'semantic'],\n ['they', 'such'],\n ['capture', 'semantic'],\n ['capture', 'they'],\n ['capture', 'syntactic'],\n ['capture', 'that'],\n ['semantic', 'syntactic'],\n ['semantic', 'capture'],\n ['semantic', 'qualities'],\n ['semantic', 'they'],\n ['syntactic', 'qualities'],\n ['syntactic', 'semantic'],\n ['syntactic', 'words'],\n ['syntactic', 'capture'],\n ['qualities', 'words'],\n ['qualities', 'syntactic'],\n ['qualities', 'such'],\n ['qualities', 'semantic'],\n ['words', 'such'],\n ['words', 'qualities'],\n ['words', 'simple'],\n ['words', 'syntactic'],\n ['such', 'simple'],\n ['such', 'words'],\n ['such', 'mathematical'],\n ['such', 'qualities'],\n ['simple', 'mathematical'],\n ['simple', 'such'],\n ['simple', 'function'],\n ['simple', 'words'],\n ['mathematical', 'function'],\n ['mathematical', 'simple'],\n ['mathematical', 'cosine'],\n ['mathematical', 'such'],\n ['function', 'cosine'],\n ['function', 'mathematical'],\n ['function', 'similarity'],\n ['function', 'simple'],\n ['cosine', 'similarity'],\n ['cosine', 'function'],\n ['cosine', 'indicate'],\n ['cosine', 'mathematical'],\n ['similarity', 'indicate'],\n ['similarity', 'cosine'],\n ['similarity', 'level'],\n ['similarity', 'function'],\n ['indicate', 'level'],\n ['indicate', 'similarity'],\n ['indicate', 'semantic'],\n ['indicate', 'cosine'],\n ['level', 'semantic'],\n ['level', 'indicate'],\n ['level', 'similarity'],\n ['level', 'similarity'],\n ['semantic', 'similarity'],\n ['semantic', 'level'],\n ['semantic', 'between'],\n ['semantic', 'indicate'],\n ['similarity', 'between'],\n ['similarity', 'semantic'],\n ['similarity', 'words'],\n ['similarity', 'level'],\n ['between', 'words'],\n ['between', 'similarity'],\n ['between', 'represented'],\n ['between', 'semantic'],\n ['words', 'represented'],\n ['words', 'between'],\n ['words', 'by'],\n ['words', 'similarity'],\n ['represented', 'by'],\n ['represented', 'words'],\n ['represented', 'those'],\n ['represented', 'between'],\n ['by', 'those'],\n ['by', 'represented'],\n ['by', 'vectors'],\n ['by', 'words'],\n ['those', 'vectors'],\n ['those', 'by'],\n ['those', 'represented'],\n ['vectors', 'those'],\n ['vectors', 'by'],\n ['group', 'related'],\n ['group', 'models'],\n ['related', 'models'],\n ['related', 'group'],\n ['related', 'that'],\n ['models', 'that'],\n ['models', 'related'],\n ['models', 'used'],\n ['models', 'group'],\n ['that', 'used'],\n ['that', 'models'],\n ['that', 'produce'],\n ['that', 'related'],\n ['used', 'produce'],\n ['used', 'that'],\n ['used', 'word'],\n ['used', 'models'],\n ['produce', 'word'],\n ['produce', 'used'],\n ['produce', 'embeddings'],\n ['produce', 'that'],\n ['word', 'embeddings'],\n ['word', 'produce'],\n ['word', 'used'],\n ['embeddings', 'word'],\n ['embeddings', 'produce'],\n ['models', 'shallow'],\n ['models', 'twolayer'],\n ['shallow', 'twolayer'],\n ['shallow', 'models'],\n ['shallow', 'neural'],\n ['twolayer', 'neural'],\n ['twolayer', 'shallow'],\n ['twolayer', 'networks'],\n ['twolayer', 'models'],\n ['neural', 'networks'],\n ['neural', 'twolayer'],\n ['neural', 'that'],\n ['neural', 'shallow'],\n ['networks', 'that'],\n ['networks', 'neural'],\n ['networks', 'trained'],\n ['networks', 'twolayer'],\n ['that', 'trained'],\n ['that', 'networks'],\n ['that', 'reconstruct'],\n ['that', 'neural'],\n ['trained', 'reconstruct'],\n ['trained', 'that'],\n ['trained', 'linguistic'],\n ['trained', 'networks'],\n ['reconstruct', 'linguistic'],\n ['reconstruct', 'trained'],\n ['reconstruct', 'contexts'],\n ['reconstruct', 'that'],\n ['linguistic', 'contexts'],\n ['linguistic', 'reconstruct'],\n ['linguistic', 'words'],\n ['linguistic', 'trained'],\n ['contexts', 'words'],\n ['contexts', 'linguistic'],\n ['contexts', 'reconstruct'],\n ['words', 'contexts'],\n ['words', 'linguistic'],\n ['takes', 'input'],\n ['takes', 'large'],\n ['input', 'large'],\n ['input', 'takes'],\n ['input', 'corpus'],\n ['large', 'corpus'],\n ['large', 'input'],\n ['large', 'text'],\n ['large', 'takes'],\n ['corpus', 'text'],\n ['corpus', 'large'],\n ['corpus', 'produces'],\n ['corpus', 'input'],\n ['text', 'produces'],\n ['text', 'corpus'],\n ['text', 'vector'],\n ['text', 'large'],\n ['produces', 'vector'],\n ['produces', 'text'],\n ['produces', 'space'],\n ['produces', 'corpus'],\n ['vector', 'space'],\n ['vector', 'produces'],\n ['vector', 'typically'],\n ['vector', 'text'],\n ['space', 'typically'],\n ['space', 'vector'],\n ['space', 'several'],\n ['space', 'produces'],\n ['typically', 'several'],\n ['typically', 'space'],\n ['typically', 'hundred'],\n ['typically', 'vector'],\n ['several', 'hundred'],\n ['several', 'typically'],\n ['several', 'dimensions'],\n ['several', 'space'],\n ['hundred', 'dimensions'],\n ['hundred', 'several'],\n ['hundred', 'each'],\n ['hundred', 'typically'],\n ['dimensions', 'each'],\n ['dimensions', 'hundred'],\n ['dimensions', 'unique'],\n ['dimensions', 'several'],\n ['each', 'unique'],\n ['each', 'dimensions'],\n ['each', 'word'],\n ['each', 'hundred'],\n ['unique', 'word'],\n ['unique', 'each'],\n ['unique', 'corpus'],\n ['unique', 'dimensions'],\n ['word', 'corpus'],\n ['word', 'unique'],\n ['word', 'being'],\n ['word', 'each'],\n ['corpus', 'being'],\n ['corpus', 'word'],\n ['corpus', 'assigned'],\n ['corpus', 'unique'],\n ['being', 'assigned'],\n ['being', 'corpus'],\n ['being', 'corresponding'],\n ['being', 'word'],\n ['assigned', 'corresponding'],\n ['assigned', 'being'],\n ['assigned', 'vector'],\n ['assigned', 'corpus'],\n ['corresponding', 'vector'],\n ['corresponding', 'assigned'],\n ['corresponding', 'space'],\n ['corresponding', 'being'],\n ['vector', 'space'],\n ['vector', 'corresponding'],\n ['vector', 'assigned'],\n ['space', 'vector'],\n ['space', 'corresponding'],\n ['utilize', 'either'],\n ['utilize', 'two'],\n ['either', 'two'],\n ['either', 'utilize'],\n ['either', 'model'],\n ['two', 'model'],\n ['two', 'either'],\n ['two', 'architectures'],\n ['two', 'utilize'],\n ['model', 'architectures'],\n ['model', 'two'],\n ['model', 'produce'],\n ['model', 'either'],\n ['architectures', 'produce'],\n ['architectures', 'model'],\n ['architectures', 'distributed'],\n ['architectures', 'two'],\n ['produce', 'distributed'],\n ['produce', 'architectures'],\n ['produce', 'representations'],\n ['produce', 'model'],\n ['distributed', 'representations'],\n ['distributed', 'produce'],\n ['distributed', 'words'],\n ['distributed', 'architectures'],\n ['representations', 'words'],\n ['representations', 'distributed'],\n ['representations', 'continuously'],\n ['representations', 'produce'],\n ['words', 'continuously'],\n ['words', 'representations'],\n ['words', 'sliding'],\n ['words', 'distributed'],\n ['continuously', 'sliding'],\n ['continuously', 'words'],\n ['continuously', 'bagofwords'],\n ['continuously', 'representations'],\n ['sliding', 'bagofwords'],\n ['sliding', 'continuously'],\n ['sliding', 'cbow'],\n ['sliding', 'words'],\n ['bagofwords', 'cbow'],\n ['bagofwords', 'sliding'],\n ['bagofwords', 'or'],\n ['bagofwords', 'continuously'],\n ['cbow', 'or'],\n ['cbow', 'bagofwords'],\n ['cbow', 'continuously'],\n ['cbow', 'sliding'],\n ['or', 'continuously'],\n ['or', 'cbow'],\n ['or', 'sliding'],\n ['or', 'bagofwords'],\n ['continuously', 'sliding'],\n ['continuously', 'or'],\n ['continuously', 'skipgram'],\n ['continuously', 'cbow'],\n ['sliding', 'skipgram'],\n ['sliding', 'continuously'],\n ['sliding', 'or'],\n ['skipgram', 'sliding'],\n ['skipgram', 'continuously'],\n ['architectures', 'considers'],\n ['architectures', 'individual'],\n ['considers', 'individual'],\n ['considers', 'architectures'],\n ['considers', 'words'],\n ['individual', 'words'],\n ['individual', 'considers'],\n ['individual', 'sliding'],\n ['individual', 'architectures'],\n ['words', 'sliding'],\n ['words', 'individual'],\n ['words', 'context'],\n ['words', 'considers'],\n ['sliding', 'context'],\n ['sliding', 'words'],\n ['sliding', 'window'],\n ['sliding', 'individual'],\n ['context', 'window'],\n ['context', 'sliding'],\n ['context', 'iterates'],\n ['context', 'words'],\n ['window', 'iterates'],\n ['window', 'context'],\n ['window', 'corpus'],\n ['window', 'sliding'],\n ['iterates', 'corpus'],\n ['iterates', 'window'],\n ['iterates', 'context'],\n ['corpus', 'iterates'],\n ['corpus', 'window'],\n ['cbow', 'viewed'],\n ['cbow', '‘fill'],\n ['viewed', '‘fill'],\n ['viewed', 'cbow'],\n ['viewed', 'blank’'],\n ['‘fill', 'blank’'],\n ['‘fill', 'viewed'],\n ['‘fill', 'task'],\n ['‘fill', 'cbow'],\n ['blank’', 'task'],\n ['blank’', '‘fill'],\n ['blank’', 'word'],\n ['blank’', 'viewed'],\n ['task', 'word'],\n ['task', 'blank’'],\n ['task', 'embedding'],\n ['task', '‘fill'],\n ['word', 'embedding'],\n ['word', 'task'],\n ['word', 'represents'],\n ['word', 'blank’'],\n ['embedding', 'represents'],\n ['embedding', 'word'],\n ['embedding', 'way'],\n ['embedding', 'task'],\n ['represents', 'way'],\n ['represents', 'embedding'],\n ['represents', 'word'],\n ['represents', 'word'],\n ['way', 'word'],\n ['way', 'represents'],\n ['way', 'influences'],\n ['way', 'embedding'],\n ['word', 'influences'],\n ['word', 'way'],\n ['word', 'relative'],\n ['word', 'represents'],\n ['influences', 'relative'],\n ['influences', 'word'],\n ['influences', 'probabilities'],\n ['influences', 'way'],\n ['relative', 'probabilities'],\n ['relative', 'influences'],\n ['relative', 'words'],\n ['relative', 'word'],\n ['probabilities', 'words'],\n ['probabilities', 'relative'],\n ['probabilities', 'context'],\n ['probabilities', 'influences'],\n ['words', 'context'],\n ['words', 'probabilities'],\n ['words', 'window'],\n ['words', 'relative'],\n ['context', 'window'],\n ['context', 'words'],\n ['context', 'probabilities'],\n ['window', 'context'],\n ['window', 'words'],\n ['words', 'which'],\n ['words', 'semantically'],\n ['which', 'semantically'],\n ['which', 'words'],\n ['which', 'similar'],\n ['semantically', 'similar'],\n ['semantically', 'which'],\n ['semantically', 'should'],\n ['semantically', 'words'],\n ['similar', 'should'],\n ['similar', 'semantically'],\n ['similar', 'influence'],\n ['similar', 'which'],\n ['should', 'influence'],\n ['should', 'similar'],\n ['should', 'probabilities'],\n ['should', 'semantically'],\n ['influence', 'probabilities'],\n ['influence', 'should'],\n ['influence', 'similar'],\n ['influence', 'similar'],\n ['probabilities', 'similar'],\n ['probabilities', 'influence'],\n ['probabilities', 'ways'],\n ['probabilities', 'should'],\n ['similar', 'ways'],\n ['similar', 'probabilities'],\n ['similar', 'semantically'],\n ['similar', 'influence'],\n ['ways', 'semantically'],\n ['ways', 'similar'],\n ['ways', 'similar'],\n ['ways', 'probabilities'],\n ['semantically', 'similar'],\n ['semantically', 'ways'],\n ['semantically', 'words'],\n ['semantically', 'similar'],\n ['similar', 'words'],\n ['similar', 'semantically'],\n ['similar', 'should'],\n ['similar', 'ways'],\n ['words', 'should'],\n ['words', 'similar'],\n ['words', 'used'],\n ['words', 'semantically'],\n ['should', 'used'],\n ['should', 'words'],\n ['should', 'similar'],\n ['should', 'similar'],\n ['used', 'similar'],\n ['used', 'should'],\n ['used', 'contexts'],\n ['used', 'words'],\n ['similar', 'contexts'],\n ['similar', 'used'],\n ['similar', 'should'],\n ['contexts', 'similar'],\n ['contexts', 'used'],\n ['order', 'context'],\n ['order', 'words'],\n ['context', 'words'],\n ['context', 'order'],\n ['context', 'not'],\n ['words', 'not'],\n ['words', 'context'],\n ['words', 'influence'],\n ['words', 'order'],\n ['not', 'influence'],\n ['not', 'words'],\n ['not', 'prediction'],\n ['not', 'context'],\n ['influence', 'prediction'],\n ['influence', 'not'],\n ['influence', 'bagofwords'],\n ['influence', 'words'],\n ['prediction', 'bagofwords'],\n ['prediction', 'influence'],\n ['prediction', 'assumption'],\n ['prediction', 'not'],\n ['bagofwords', 'assumption'],\n ['bagofwords', 'prediction'],\n ['bagofwords', 'influence'],\n ['assumption', 'bagofwords'],\n ['assumption', 'prediction'],\n ['continuous', 'skipgram'],\n ['continuous', 'architecture'],\n ['skipgram', 'architecture'],\n ['skipgram', 'continuous'],\n ['skipgram', 'model'],\n ['architecture', 'model'],\n ['architecture', 'skipgram'],\n ['architecture', 'uses'],\n ['architecture', 'continuous'],\n ['model', 'uses'],\n ['model', 'architecture'],\n ['model', 'current'],\n ['model', 'skipgram'],\n ['uses', 'current'],\n ['uses', 'model'],\n ['uses', 'word'],\n ['uses', 'architecture'],\n ['current', 'word'],\n ['current', 'uses'],\n ['current', 'predict'],\n ['current', 'model'],\n ['word', 'predict'],\n ['word', 'current'],\n ['word', 'surrounding'],\n ['word', 'uses'],\n ['predict', 'surrounding'],\n ['predict', 'word'],\n ['predict', 'window'],\n ['predict', 'current'],\n ['surrounding', 'window'],\n ['surrounding', 'predict'],\n ['surrounding', 'context'],\n ['surrounding', 'word'],\n ['window', 'context'],\n ['window', 'surrounding'],\n ['window', 'words'],\n ['window', 'predict'],\n ['context', 'words'],\n ['context', 'window'],\n ['context', 'surrounding'],\n ['words', 'context'],\n ['words', 'window'],\n ['skipgram', 'architecture'],\n ['skipgram', 'weighs'],\n ['architecture', 'weighs'],\n ['architecture', 'skipgram'],\n ['architecture', 'nearby'],\n ['weighs', 'nearby'],\n ['weighs', 'architecture'],\n ['weighs', 'context'],\n ['weighs', 'skipgram'],\n ['nearby', 'context'],\n ['nearby', 'weighs'],\n ['nearby', 'words'],\n ['nearby', 'architecture'],\n ['context', 'words'],\n ['context', 'nearby'],\n ['context', 'more'],\n ['context', 'weighs'],\n ['words', 'more'],\n ['words', 'context'],\n ['words', 'heavily'],\n ['words', 'nearby'],\n ['more', 'heavily'],\n ['more', 'words'],\n ['more', 'than'],\n ['more', 'context'],\n ['heavily', 'than'],\n ['heavily', 'more'],\n ['heavily', 'more'],\n ['heavily', 'words'],\n ['than', 'more'],\n ['than', 'heavily'],\n ['than', 'distant'],\n ['than', 'more'],\n ['more', 'distant'],\n ['more', 'than'],\n ['more', 'context'],\n ['more', 'heavily'],\n ['distant', 'context'],\n ['distant', 'more'],\n ['distant', 'words'],\n ['distant', 'than'],\n ['context', 'words'],\n ['context', 'distant'],\n ['context', 'more'],\n ['words', 'context'],\n ['words', 'distant'],\n ['according', 'authors'],\n ['according', 'cbow'],\n ['authors', 'cbow'],\n ['authors', 'according'],\n ['authors', 'faster'],\n ['cbow', 'faster'],\n ['cbow', 'authors'],\n ['cbow', 'while'],\n ['cbow', 'according'],\n ['faster', 'while'],\n ['faster', 'cbow'],\n ['faster', 'skipgram'],\n ['faster', 'authors'],\n ['while', 'skipgram'],\n ['while', 'faster'],\n ['while', 'better'],\n ['while', 'cbow'],\n ['skipgram', 'better'],\n ['skipgram', 'while'],\n ['skipgram', 'job'],\n ['skipgram', 'faster'],\n ['better', 'job'],\n ['better', 'skipgram'],\n ['better', 'infrequent'],\n ['better', 'while'],\n ['job', 'infrequent'],\n ['job', 'better'],\n ['job', 'words'],\n ['job', 'skipgram'],\n ['infrequent', 'words'],\n ['infrequent', 'job'],\n ['infrequent', 'better'],\n ['words', 'infrequent'],\n ['words', 'job'],\n ['after', 'model'],\n ['after', 'has'],\n ['model', 'has'],\n ['model', 'after'],\n ['model', 'trained'],\n ['has', 'trained'],\n ['has', 'model'],\n ['has', 'learned'],\n ['has', 'after'],\n ['trained', 'learned'],\n ['trained', 'has'],\n ['trained', 'word'],\n ['trained', 'model'],\n ['learned', 'word'],\n ['learned', 'trained'],\n ['learned', 'embeddings'],\n ['learned', 'has'],\n ['word', 'embeddings'],\n ['word', 'learned'],\n ['word', 'positioned'],\n ['word', 'trained'],\n ['embeddings', 'positioned'],\n ['embeddings', 'word'],\n ['embeddings', 'vector'],\n ['embeddings', 'learned'],\n ['positioned', 'vector'],\n ['positioned', 'embeddings'],\n ['positioned', 'space'],\n ['positioned', 'word'],\n ['vector', 'space'],\n ['vector', 'positioned'],\n ['vector', 'such'],\n ['vector', 'embeddings'],\n ['space', 'such'],\n ['space', 'vector'],\n ['space', 'that'],\n ['space', 'positioned'],\n ['such', 'that'],\n ['such', 'space'],\n ['such', 'words'],\n ['such', 'vector'],\n ['that', 'words'],\n ['that', 'such'],\n ['that', 'that'],\n ['that', 'space'],\n ['words', 'that'],\n ['words', 'that'],\n ['words', 'share'],\n ['words', 'such'],\n ['that', 'share'],\n ['that', 'words'],\n ['that', 'common'],\n ['that', 'that'],\n ['share', 'common'],\n ['share', 'that'],\n ['share', 'contexts'],\n ['share', 'words'],\n ['common', 'contexts'],\n ['common', 'share'],\n ['common', 'corpus'],\n ['common', 'that'],\n ['contexts', 'corpus'],\n ['contexts', 'common'],\n ['contexts', 'that'],\n ['contexts', 'share'],\n ['corpus', 'that'],\n ['corpus', 'contexts'],\n ['corpus', 'words'],\n ['corpus', 'common'],\n ['that', 'words'],\n ['that', 'corpus'],\n ['that', 'that'],\n ['that', 'contexts'],\n ['words', 'that'],\n ['words', 'that'],\n ['words', 'semantically'],\n ['words', 'corpus'],\n ['that', 'semantically'],\n ['that', 'words'],\n ['that', 'syntactically'],\n ['that', 'that'],\n ['semantically', 'syntactically'],\n ['semantically', 'that'],\n ['semantically', 'similar'],\n ['semantically', 'words'],\n ['syntactically', 'similar'],\n ['syntactically', 'semantically'],\n ['syntactically', 'located'],\n ['syntactically', 'that'],\n ['similar', 'located'],\n ['similar', 'syntactically'],\n ['similar', 'close'],\n ['similar', 'semantically'],\n ['located', 'close'],\n ['located', 'similar'],\n ['located', 'one'],\n ['located', 'syntactically'],\n ['close', 'one'],\n ['close', 'located'],\n ['close', 'another'],\n ['close', 'similar'],\n ['one', 'another'],\n ['one', 'close'],\n ['one', 'space'],\n ['one', 'located'],\n ['another', 'space'],\n ['another', 'one'],\n ['another', 'close'],\n ['space', 'another'],\n ['space', 'one'],\n ['more', 'dissimilar'],\n ['more', 'words'],\n ['dissimilar', 'words'],\n ['dissimilar', 'more'],\n ['dissimilar', 'located'],\n ['words', 'located'],\n ['words', 'dissimilar'],\n ['words', 'farther'],\n ['words', 'more'],\n ['located', 'farther'],\n ['located', 'words'],\n ['located', 'one'],\n ['located', 'dissimilar'],\n ['farther', 'one'],\n ['farther', 'located'],\n ['farther', 'another'],\n ['farther', 'words'],\n ['one', 'another'],\n ['one', 'farther'],\n ['one', 'space'],\n ['one', 'located'],\n ['another', 'space'],\n ['another', 'one'],\n ['another', 'farther'],\n ['space', 'another'],\n ['space', 'one']]"
     },
     "metadata": {}
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "all_text1",
   "metadata": {
    "id": "a67f4701aaea78f4",
    "outputId": "cd575761-f3bc-450d-c051-a334ea96c2d7",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:27:00.094294400Z",
     "start_time": "2023-12-03T05:26:59.536870700Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:37.139287Z",
     "iopub.execute_input": "2025-07-05T17:37:37.139452Z",
     "iopub.status.idle": "2025-07-05T17:37:37.156155Z",
     "shell.execute_reply.started": "2025-07-05T17:37:37.139438Z",
     "shell.execute_reply": "2025-07-05T17:37:37.15561Z"
    }
   },
   "outputs": [
    {
     "execution_count": 12,
     "output_type": "execute_result",
     "data": {
      "text/plain": "['skipgram',\n 'one',\n 'more',\n 'syntactic',\n 'prediction',\n 'better',\n 'each',\n 'while',\n 'list',\n 'implies',\n 'takes',\n 'represented',\n 'between',\n 'farther',\n 'function',\n 'reconstruct',\n 'algorithm',\n 'typically',\n 'dimensions',\n 'either',\n 'models',\n 'word',\n 'suggest',\n 'vector',\n 'which',\n 'predict',\n 'task',\n 'vectors',\n 'by',\n 'language',\n 'located',\n 'bagofwords',\n 'after',\n 'nlp',\n 'embeddings',\n 'similarity',\n 'influence',\n 'words',\n 'chosen',\n 'window',\n 'additional',\n 'another',\n 'relative',\n 'space',\n 'input',\n 'model',\n 'those',\n 'carefully',\n 'large',\n 'probabilities',\n 'continuously',\n 'capture',\n 'twolayer',\n 'utilize',\n 'should',\n 'ways',\n 'technique',\n 'learn',\n 'qualities',\n 'used',\n 'published',\n 'or',\n 'influences',\n 'neural',\n 'partial',\n 'several',\n 'name',\n 'level',\n 'considers',\n 'related',\n 'heavily',\n 'nearby',\n 'network',\n 'individual',\n 'distant',\n 'natural',\n 'processing',\n 'not',\n 'cosine',\n 'syntactically',\n 'sentence',\n 'embedding',\n 'trained',\n 'semantically',\n 'produces',\n 'common',\n 'associations',\n 'shallow',\n 'text',\n 'positioned',\n 'once',\n 'learned',\n 'surrounding',\n 'that',\n '‘fill',\n 'networks',\n 'being',\n 'assumption',\n 'share',\n 'distinct',\n 'close',\n 'according',\n 'produce',\n 'uses',\n 'assigned',\n 'continuous',\n 'sliding',\n 'order',\n 'such',\n 'iterates',\n 'blank’',\n 'weighs',\n 'numbers',\n 'unique',\n 'similar',\n 'has',\n 'infrequent',\n 'called',\n 'synonymous',\n 'simple',\n 'corresponding',\n 'semantic',\n 'they',\n 'way',\n 'faster',\n 'representations',\n 'than',\n 'current',\n 'indicate',\n 'distributed',\n 'hundred',\n 'particular',\n 'authors',\n 'corpus',\n 'detect',\n 'cbow',\n 'architectures',\n 'mathematical',\n 'contexts',\n 'group',\n 'dissimilar',\n 'linguistic',\n 'viewed',\n 'context',\n 'represents',\n 'two',\n 'job',\n 'architecture']"
     },
     "metadata": {}
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "def create_unique_word_dict(TEXT: list):\n    \"\"\"\n    A method that creates a dictionary where the keys are unique words\n    and key values are indices\n    \"\"\"\n    # Getting all the unique words from our text and sorting them alphabetically\n    Words = list(set(TEXT))\n    Words.sort()\n\n    # Creating the dictionary for the unique words\n    unique_word_dict = {}\n    for i, Word in enumerate(Words):\n        unique_word_dict.update({Word: i})\n\n    return unique_word_dict, len(unique_word_dict)",
   "metadata": {
    "papermill": {
     "duration": 0.019596,
     "end_time": "2023-12-01T13:37:43.642784",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.623188",
     "status": "completed"
    },
    "tags": [],
    "id": "9e0cd93f",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:27:00.097299100Z",
     "start_time": "2023-12-03T05:26:59.553417800Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:37.156892Z",
     "iopub.execute_input": "2025-07-05T17:37:37.157109Z",
     "iopub.status.idle": "2025-07-05T17:37:37.172329Z",
     "shell.execute_reply.started": "2025-07-05T17:37:37.157086Z",
     "shell.execute_reply": "2025-07-05T17:37:37.171643Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "unique_word_dict1, number_of_unique_words1 = create_unique_word_dict(all_text1)",
   "metadata": {
    "papermill": {
     "duration": 0.018563,
     "end_time": "2023-12-01T13:37:43.669969",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.651406",
     "status": "completed"
    },
    "tags": [],
    "id": "3e144546",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:27:00.098300600Z",
     "start_time": "2023-12-03T05:26:59.561146100Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:37.173034Z",
     "iopub.execute_input": "2025-07-05T17:37:37.17363Z",
     "iopub.status.idle": "2025-07-05T17:37:37.187264Z",
     "shell.execute_reply.started": "2025-07-05T17:37:37.173611Z",
     "shell.execute_reply": "2025-07-05T17:37:37.186601Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "unique_word_dict1",
   "metadata": {
    "id": "cc9da03ca68b1eab",
    "outputId": "31fe72a3-9d92-4e58-fcf3-a11595d8ee2a",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:27:00.145295800Z",
     "start_time": "2023-12-03T05:26:59.575195400Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:37.188Z",
     "iopub.execute_input": "2025-07-05T17:37:37.188237Z",
     "iopub.status.idle": "2025-07-05T17:37:37.205384Z",
     "shell.execute_reply.started": "2025-07-05T17:37:37.188216Z",
     "shell.execute_reply": "2025-07-05T17:37:37.204792Z"
    }
   },
   "outputs": [
    {
     "execution_count": 15,
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'according': 0,\n 'additional': 1,\n 'after': 2,\n 'algorithm': 3,\n 'another': 4,\n 'architecture': 5,\n 'architectures': 6,\n 'assigned': 7,\n 'associations': 8,\n 'assumption': 9,\n 'authors': 10,\n 'bagofwords': 11,\n 'being': 12,\n 'better': 13,\n 'between': 14,\n 'blank’': 15,\n 'by': 16,\n 'called': 17,\n 'capture': 18,\n 'carefully': 19,\n 'cbow': 20,\n 'chosen': 21,\n 'close': 22,\n 'common': 23,\n 'considers': 24,\n 'context': 25,\n 'contexts': 26,\n 'continuous': 27,\n 'continuously': 28,\n 'corpus': 29,\n 'corresponding': 30,\n 'cosine': 31,\n 'current': 32,\n 'detect': 33,\n 'dimensions': 34,\n 'dissimilar': 35,\n 'distant': 36,\n 'distinct': 37,\n 'distributed': 38,\n 'each': 39,\n 'either': 40,\n 'embedding': 41,\n 'embeddings': 42,\n 'farther': 43,\n 'faster': 44,\n 'function': 45,\n 'group': 46,\n 'has': 47,\n 'heavily': 48,\n 'hundred': 49,\n 'implies': 50,\n 'indicate': 51,\n 'individual': 52,\n 'influence': 53,\n 'influences': 54,\n 'infrequent': 55,\n 'input': 56,\n 'iterates': 57,\n 'job': 58,\n 'language': 59,\n 'large': 60,\n 'learn': 61,\n 'learned': 62,\n 'level': 63,\n 'linguistic': 64,\n 'list': 65,\n 'located': 66,\n 'mathematical': 67,\n 'model': 68,\n 'models': 69,\n 'more': 70,\n 'name': 71,\n 'natural': 72,\n 'nearby': 73,\n 'network': 74,\n 'networks': 75,\n 'neural': 76,\n 'nlp': 77,\n 'not': 78,\n 'numbers': 79,\n 'once': 80,\n 'one': 81,\n 'or': 82,\n 'order': 83,\n 'partial': 84,\n 'particular': 85,\n 'positioned': 86,\n 'predict': 87,\n 'prediction': 88,\n 'probabilities': 89,\n 'processing': 90,\n 'produce': 91,\n 'produces': 92,\n 'published': 93,\n 'qualities': 94,\n 'reconstruct': 95,\n 'related': 96,\n 'relative': 97,\n 'representations': 98,\n 'represented': 99,\n 'represents': 100,\n 'semantic': 101,\n 'semantically': 102,\n 'sentence': 103,\n 'several': 104,\n 'shallow': 105,\n 'share': 106,\n 'should': 107,\n 'similar': 108,\n 'similarity': 109,\n 'simple': 110,\n 'skipgram': 111,\n 'sliding': 112,\n 'space': 113,\n 'such': 114,\n 'suggest': 115,\n 'surrounding': 116,\n 'synonymous': 117,\n 'syntactic': 118,\n 'syntactically': 119,\n 'takes': 120,\n 'task': 121,\n 'technique': 122,\n 'text': 123,\n 'than': 124,\n 'that': 125,\n 'they': 126,\n 'those': 127,\n 'trained': 128,\n 'two': 129,\n 'twolayer': 130,\n 'typically': 131,\n 'unique': 132,\n 'used': 133,\n 'uses': 134,\n 'utilize': 135,\n 'vector': 136,\n 'vectors': 137,\n 'viewed': 138,\n 'way': 139,\n 'ways': 140,\n 'weighs': 141,\n 'which': 142,\n 'while': 143,\n 'window': 144,\n 'word': 145,\n 'words': 146,\n '‘fill': 147}"
     },
     "metadata": {}
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": "number_of_unique_words1",
   "metadata": {
    "id": "2b483ffe7b804fdc",
    "outputId": "50f081ef-b1bc-43d8-8b26-18dc66a5dd71",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:27:00.147293400Z",
     "start_time": "2023-12-03T05:26:59.591516900Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:37.206007Z",
     "iopub.execute_input": "2025-07-05T17:37:37.206157Z",
     "iopub.status.idle": "2025-07-05T17:37:37.220581Z",
     "shell.execute_reply.started": "2025-07-05T17:37:37.206146Z",
     "shell.execute_reply": "2025-07-05T17:37:37.219842Z"
    }
   },
   "outputs": [
    {
     "execution_count": 16,
     "output_type": "execute_result",
     "data": {
      "text/plain": "148"
     },
     "metadata": {}
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": "# Getting all the unique words\nunique_word1 = list(unique_word_dict1.keys())",
   "metadata": {
    "papermill": {
     "duration": 0.018983,
     "end_time": "2023-12-01T13:37:43.697821",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.678838",
     "status": "completed"
    },
    "tags": [],
    "id": "9065ff09",
    "outputId": "033fb3f4-a80c-4e75-e2e9-d00a76f88e45",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:27:00.148293500Z",
     "start_time": "2023-12-03T05:26:59.601295700Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:37.221575Z",
     "iopub.execute_input": "2025-07-05T17:37:37.222063Z",
     "iopub.status.idle": "2025-07-05T17:37:37.236389Z",
     "shell.execute_reply.started": "2025-07-05T17:37:37.222048Z",
     "shell.execute_reply": "2025-07-05T17:37:37.235877Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": "unique_word1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:27:00.149297100Z",
     "start_time": "2023-12-03T05:26:59.610051200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:37.237168Z",
     "iopub.execute_input": "2025-07-05T17:37:37.237394Z",
     "iopub.status.idle": "2025-07-05T17:37:37.255181Z",
     "shell.execute_reply.started": "2025-07-05T17:37:37.23738Z",
     "shell.execute_reply": "2025-07-05T17:37:37.254677Z"
    }
   },
   "outputs": [
    {
     "execution_count": 18,
     "output_type": "execute_result",
     "data": {
      "text/plain": "['according',\n 'additional',\n 'after',\n 'algorithm',\n 'another',\n 'architecture',\n 'architectures',\n 'assigned',\n 'associations',\n 'assumption',\n 'authors',\n 'bagofwords',\n 'being',\n 'better',\n 'between',\n 'blank’',\n 'by',\n 'called',\n 'capture',\n 'carefully',\n 'cbow',\n 'chosen',\n 'close',\n 'common',\n 'considers',\n 'context',\n 'contexts',\n 'continuous',\n 'continuously',\n 'corpus',\n 'corresponding',\n 'cosine',\n 'current',\n 'detect',\n 'dimensions',\n 'dissimilar',\n 'distant',\n 'distinct',\n 'distributed',\n 'each',\n 'either',\n 'embedding',\n 'embeddings',\n 'farther',\n 'faster',\n 'function',\n 'group',\n 'has',\n 'heavily',\n 'hundred',\n 'implies',\n 'indicate',\n 'individual',\n 'influence',\n 'influences',\n 'infrequent',\n 'input',\n 'iterates',\n 'job',\n 'language',\n 'large',\n 'learn',\n 'learned',\n 'level',\n 'linguistic',\n 'list',\n 'located',\n 'mathematical',\n 'model',\n 'models',\n 'more',\n 'name',\n 'natural',\n 'nearby',\n 'network',\n 'networks',\n 'neural',\n 'nlp',\n 'not',\n 'numbers',\n 'once',\n 'one',\n 'or',\n 'order',\n 'partial',\n 'particular',\n 'positioned',\n 'predict',\n 'prediction',\n 'probabilities',\n 'processing',\n 'produce',\n 'produces',\n 'published',\n 'qualities',\n 'reconstruct',\n 'related',\n 'relative',\n 'representations',\n 'represented',\n 'represents',\n 'semantic',\n 'semantically',\n 'sentence',\n 'several',\n 'shallow',\n 'share',\n 'should',\n 'similar',\n 'similarity',\n 'simple',\n 'skipgram',\n 'sliding',\n 'space',\n 'such',\n 'suggest',\n 'surrounding',\n 'synonymous',\n 'syntactic',\n 'syntactically',\n 'takes',\n 'task',\n 'technique',\n 'text',\n 'than',\n 'that',\n 'they',\n 'those',\n 'trained',\n 'two',\n 'twolayer',\n 'typically',\n 'unique',\n 'used',\n 'uses',\n 'utilize',\n 'vector',\n 'vectors',\n 'viewed',\n 'way',\n 'ways',\n 'weighs',\n 'which',\n 'while',\n 'window',\n 'word',\n 'words',\n '‘fill']"
     },
     "metadata": {}
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": "def one_hot_representation(word_lists, number_of_unique_words, unique_word_dict):\n    # Creating the X and Y matrices using one hot encoding\n    X = []\n    Y = []\n\n    for i, word_list in tqdm(enumerate(word_lists)):\n        # Getting the indices\n        main_word_index = unique_word_dict.get(word_list[0])\n        context_word_index = unique_word_dict.get(word_list[1])\n\n        print(word_list)\n        print(word_list[0], main_word_index)\n        print(word_list[1], context_word_index)\n\n        # Creating the placeholders\n        X_row = np.zeros(number_of_unique_words)\n        Y_row = np.zeros(number_of_unique_words)\n\n        # One hot encoding the main word\n        X_row[main_word_index] = 1\n\n        # One hot encoding the Y matrix words\n        Y_row[context_word_index] = 1\n\n        # Appending to the main matrices\n        X.append(X_row)\n        Y.append(Y_row)\n    return X, Y",
   "metadata": {
    "papermill": {
     "duration": 0.032739,
     "end_time": "2023-12-01T13:37:43.739535",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.706796",
     "status": "completed"
    },
    "tags": [],
    "id": "3862ca2d",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:27:00.150293500Z",
     "start_time": "2023-12-03T05:26:59.630551600Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:37.255862Z",
     "iopub.execute_input": "2025-07-05T17:37:37.256086Z",
     "iopub.status.idle": "2025-07-05T17:37:37.271614Z",
     "shell.execute_reply.started": "2025-07-05T17:37:37.256063Z",
     "shell.execute_reply": "2025-07-05T17:37:37.271124Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "matrix_x1, matrix_y1 = one_hot_representation(\n",
    "    word_lists1, number_of_unique_words1, unique_word_dict1\n",
    ")"
   ],
   "metadata": {
    "id": "IvFkNXbJ17bV",
    "outputId": "180d7e35-7ac0-4aa8-f587-06323b122964",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:27:00.168289900Z",
     "start_time": "2023-12-03T05:26:59.638072300Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:37.272328Z",
     "iopub.execute_input": "2025-07-05T17:37:37.272572Z",
     "iopub.status.idle": "2025-07-05T17:37:37.32083Z",
     "shell.execute_reply.started": "2025-07-05T17:37:37.272553Z",
     "shell.execute_reply": "2025-07-05T17:37:37.320097Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "0it [00:00, ?it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "['technique', 'natural']\ntechnique 122\nnatural 72\n['technique', 'language']\ntechnique 122\nlanguage 59\n['natural', 'language']\nnatural 72\nlanguage 59\n['natural', 'technique']\nnatural 72\ntechnique 122\n['natural', 'processing']\nnatural 72\nprocessing 90\n['language', 'processing']\nlanguage 59\nprocessing 90\n['language', 'natural']\nlanguage 59\nnatural 72\n['language', 'nlp']\nlanguage 59\nnlp 77\n['language', 'technique']\nlanguage 59\ntechnique 122\n['processing', 'nlp']\nprocessing 90\nnlp 77\n['processing', 'language']\nprocessing 90\nlanguage 59\n['processing', 'published']\nprocessing 90\npublished 93\n['processing', 'natural']\nprocessing 90\nnatural 72\n['nlp', 'published']\nnlp 77\npublished 93\n['nlp', 'processing']\nnlp 77\nprocessing 90\n['nlp', 'language']\nnlp 77\nlanguage 59\n['published', 'nlp']\npublished 93\nnlp 77\n['published', 'processing']\npublished 93\nprocessing 90\n['algorithm', 'uses']\nalgorithm 3\nuses 134\n['algorithm', 'neural']\nalgorithm 3\nneural 76\n['uses', 'neural']\nuses 134\nneural 76\n['uses', 'algorithm']\nuses 134\nalgorithm 3\n['uses', 'network']\nuses 134\nnetwork 74\n['neural', 'network']\nneural 76\nnetwork 74\n['neural', 'uses']\nneural 76\nuses 134\n['neural', 'model']\nneural 76\nmodel 68\n['neural', 'algorithm']\nneural 76\nalgorithm 3\n['network', 'model']\nnetwork 74\nmodel 68\n['network', 'neural']\nnetwork 74\nneural 76\n['network', 'learn']\nnetwork 74\nlearn 61\n['network', 'uses']\nnetwork 74\nuses 134\n['model', 'learn']\nmodel 68\nlearn 61\n['model', 'network']\nmodel 68\nnetwork 74\n['model', 'word']\nmodel 68\nword 145\n['model', 'neural']\nmodel 68\nneural 76\n['learn', 'word']\nlearn 61\nword 145\n['learn', 'model']\nlearn 61\nmodel 68\n['learn', 'associations']\nlearn 61\nassociations 8\n['learn', 'network']\nlearn 61\nnetwork 74\n['word', 'associations']\nword 145\nassociations 8\n['word', 'learn']\nword 145\nlearn 61\n['word', 'large']\nword 145\nlarge 60\n['word', 'model']\nword 145\nmodel 68\n['associations', 'large']\nassociations 8\nlarge 60\n['associations', 'word']\nassociations 8\nword 145\n['associations', 'corpus']\nassociations 8\ncorpus 29\n['associations', 'learn']\nassociations 8\nlearn 61\n['large', 'corpus']\nlarge 60\ncorpus 29\n['large', 'associations']\nlarge 60\nassociations 8\n['large', 'text']\nlarge 60\ntext 123\n['large', 'word']\nlarge 60\nword 145\n['corpus', 'text']\ncorpus 29\ntext 123\n['corpus', 'large']\ncorpus 29\nlarge 60\n['corpus', 'associations']\ncorpus 29\nassociations 8\n['text', 'corpus']\ntext 123\ncorpus 29\n['text', 'large']\ntext 123\nlarge 60\n['once', 'trained']\nonce 80\ntrained 128\n['once', 'such']\nonce 80\nsuch 114\n['trained', 'such']\ntrained 128\nsuch 114\n['trained', 'once']\ntrained 128\nonce 80\n['trained', 'model']\ntrained 128\nmodel 68\n['such', 'model']\nsuch 114\nmodel 68\n['such', 'trained']\nsuch 114\ntrained 128\n['such', 'detect']\nsuch 114\ndetect 33\n['such', 'once']\nsuch 114\nonce 80\n['model', 'detect']\nmodel 68\ndetect 33\n['model', 'such']\nmodel 68\nsuch 114\n['model', 'synonymous']\nmodel 68\nsynonymous 117\n['model', 'trained']\nmodel 68\ntrained 128\n['detect', 'synonymous']\ndetect 33\nsynonymous 117\n['detect', 'model']\ndetect 33\nmodel 68\n['detect', 'words']\ndetect 33\nwords 146\n['detect', 'such']\ndetect 33\nsuch 114\n['synonymous', 'words']\nsynonymous 117\nwords 146\n['synonymous', 'detect']\nsynonymous 117\ndetect 33\n['synonymous', 'or']\nsynonymous 117\nor 82\n['synonymous', 'model']\nsynonymous 117\nmodel 68\n['words', 'or']\nwords 146\nor 82\n['words', 'synonymous']\nwords 146\nsynonymous 117\n['words', 'suggest']\nwords 146\nsuggest 115\n['words', 'detect']\nwords 146\ndetect 33\n['or', 'suggest']\nor 82\nsuggest 115\n['or', 'words']\nor 82\nwords 146\n['or', 'additional']\nor 82\nadditional 1\n['or', 'synonymous']\nor 82\nsynonymous 117\n['suggest', 'additional']\nsuggest 115\nadditional 1\n['suggest', 'or']\nsuggest 115\nor 82\n['suggest', 'words']\nsuggest 115\nwords 146\n['suggest', 'words']\nsuggest 115\nwords 146\n['additional', 'words']\nadditional 1\nwords 146\n['additional', 'suggest']\nadditional 1\nsuggest 115\n['additional', 'partial']\nadditional 1\npartial 84\n['additional', 'or']\nadditional 1\nor 82\n['words', 'partial']\nwords 146\npartial 84\n['words', 'additional']\nwords 146\nadditional 1\n['words', 'sentence']\nwords 146\nsentence 103\n['words', 'suggest']\nwords 146\nsuggest 115\n['partial', 'sentence']\npartial 84\nsentence 103\n['partial', 'words']\npartial 84\nwords 146\n['partial', 'additional']\npartial 84\nadditional 1\n['sentence', 'partial']\nsentence 103\npartial 84\n['sentence', 'words']\nsentence 103\nwords 146\n['name', 'implies']\nname 71\nimplies 50\n['name', 'represents']\nname 71\nrepresents 100\n['implies', 'represents']\nimplies 50\nrepresents 100\n['implies', 'name']\nimplies 50\nname 71\n['implies', 'each']\nimplies 50\neach 39\n['represents', 'each']\nrepresents 100\neach 39\n['represents', 'implies']\nrepresents 100\nimplies 50\n['represents', 'distinct']\nrepresents 100\ndistinct 37\n['represents', 'name']\nrepresents 100\nname 71\n['each', 'distinct']\neach 39\ndistinct 37\n['each', 'represents']\neach 39\nrepresents 100\n['each', 'word']\neach 39\nword 145\n['each', 'implies']\neach 39\nimplies 50\n['distinct', 'word']\ndistinct 37\nword 145\n['distinct', 'each']\ndistinct 37\neach 39\n['distinct', 'particular']\ndistinct 37\nparticular 85\n['distinct', 'represents']\ndistinct 37\nrepresents 100\n['word', 'particular']\nword 145\nparticular 85\n['word', 'distinct']\nword 145\ndistinct 37\n['word', 'list']\nword 145\nlist 65\n['word', 'each']\nword 145\neach 39\n['particular', 'list']\nparticular 85\nlist 65\n['particular', 'word']\nparticular 85\nword 145\n['particular', 'numbers']\nparticular 85\nnumbers 79\n['particular', 'distinct']\nparticular 85\ndistinct 37\n['list', 'numbers']\nlist 65\nnumbers 79\n['list', 'particular']\nlist 65\nparticular 85\n['list', 'called']\nlist 65\ncalled 17\n['list', 'word']\nlist 65\nword 145\n['numbers', 'called']\nnumbers 79\ncalled 17\n['numbers', 'list']\nnumbers 79\nlist 65\n['numbers', 'vector']\nnumbers 79\nvector 136\n['numbers', 'particular']\nnumbers 79\nparticular 85\n['called', 'vector']\ncalled 17\nvector 136\n['called', 'numbers']\ncalled 17\nnumbers 79\n['called', 'list']\ncalled 17\nlist 65\n['vector', 'called']\nvector 136\ncalled 17\n['vector', 'numbers']\nvector 136\nnumbers 79\n['vectors', 'chosen']\nvectors 137\nchosen 21\n['vectors', 'carefully']\nvectors 137\ncarefully 19\n['chosen', 'carefully']\nchosen 21\ncarefully 19\n['chosen', 'vectors']\nchosen 21\nvectors 137\n['chosen', 'such']\nchosen 21\nsuch 114\n['carefully', 'such']\ncarefully 19\nsuch 114\n['carefully', 'chosen']\ncarefully 19\nchosen 21\n['carefully', 'that']\ncarefully 19\nthat 125\n['carefully', 'vectors']\ncarefully 19\nvectors 137\n['such', 'that']\nsuch 114\nthat 125\n['such', 'carefully']\nsuch 114\ncarefully 19\n['such', 'they']\nsuch 114\nthey 126\n['such', 'chosen']\nsuch 114\nchosen 21\n['that', 'they']\nthat 125\nthey 126\n['that', 'such']\nthat 125\nsuch 114\n['that', 'capture']\nthat 125\ncapture 18\n['that', 'carefully']\nthat 125\ncarefully 19\n['they', 'capture']\nthey 126\ncapture 18\n['they', 'that']\nthey 126\nthat 125\n['they', 'semantic']\nthey 126\nsemantic 101\n['they', 'such']\nthey 126\nsuch 114\n['capture', 'semantic']\ncapture 18\nsemantic 101\n['capture', 'they']\ncapture 18\nthey 126\n['capture', 'syntactic']\ncapture 18\nsyntactic 118\n['capture', 'that']\ncapture 18\nthat 125\n['semantic', 'syntactic']\nsemantic 101\nsyntactic 118\n['semantic', 'capture']\nsemantic 101\ncapture 18\n['semantic', 'qualities']\nsemantic 101\nqualities 94\n['semantic', 'they']\nsemantic 101\nthey 126\n['syntactic', 'qualities']\nsyntactic 118\nqualities 94\n['syntactic', 'semantic']\nsyntactic 118\nsemantic 101\n['syntactic', 'words']\nsyntactic 118\nwords 146\n['syntactic', 'capture']\nsyntactic 118\ncapture 18\n['qualities', 'words']\nqualities 94\nwords 146\n['qualities', 'syntactic']\nqualities 94\nsyntactic 118\n['qualities', 'such']\nqualities 94\nsuch 114\n['qualities', 'semantic']\nqualities 94\nsemantic 101\n['words', 'such']\nwords 146\nsuch 114\n['words', 'qualities']\nwords 146\nqualities 94\n['words', 'simple']\nwords 146\nsimple 110\n['words', 'syntactic']\nwords 146\nsyntactic 118\n['such', 'simple']\nsuch 114\nsimple 110\n['such', 'words']\nsuch 114\nwords 146\n['such', 'mathematical']\nsuch 114\nmathematical 67\n['such', 'qualities']\nsuch 114\nqualities 94\n['simple', 'mathematical']\nsimple 110\nmathematical 67\n['simple', 'such']\nsimple 110\nsuch 114\n['simple', 'function']\nsimple 110\nfunction 45\n['simple', 'words']\nsimple 110\nwords 146\n['mathematical', 'function']\nmathematical 67\nfunction 45\n['mathematical', 'simple']\nmathematical 67\nsimple 110\n['mathematical', 'cosine']\nmathematical 67\ncosine 31\n['mathematical', 'such']\nmathematical 67\nsuch 114\n['function', 'cosine']\nfunction 45\ncosine 31\n['function', 'mathematical']\nfunction 45\nmathematical 67\n['function', 'similarity']\nfunction 45\nsimilarity 109\n['function', 'simple']\nfunction 45\nsimple 110\n['cosine', 'similarity']\ncosine 31\nsimilarity 109\n['cosine', 'function']\ncosine 31\nfunction 45\n['cosine', 'indicate']\ncosine 31\nindicate 51\n['cosine', 'mathematical']\ncosine 31\nmathematical 67\n['similarity', 'indicate']\nsimilarity 109\nindicate 51\n['similarity', 'cosine']\nsimilarity 109\ncosine 31\n['similarity', 'level']\nsimilarity 109\nlevel 63\n['similarity', 'function']\nsimilarity 109\nfunction 45\n['indicate', 'level']\nindicate 51\nlevel 63\n['indicate', 'similarity']\nindicate 51\nsimilarity 109\n['indicate', 'semantic']\nindicate 51\nsemantic 101\n['indicate', 'cosine']\nindicate 51\ncosine 31\n['level', 'semantic']\nlevel 63\nsemantic 101\n['level', 'indicate']\nlevel 63\nindicate 51\n['level', 'similarity']\nlevel 63\nsimilarity 109\n['level', 'similarity']\nlevel 63\nsimilarity 109\n['semantic', 'similarity']\nsemantic 101\nsimilarity 109\n['semantic', 'level']\nsemantic 101\nlevel 63\n['semantic', 'between']\nsemantic 101\nbetween 14\n['semantic', 'indicate']\nsemantic 101\nindicate 51\n['similarity', 'between']\nsimilarity 109\nbetween 14\n['similarity', 'semantic']\nsimilarity 109\nsemantic 101\n['similarity', 'words']\nsimilarity 109\nwords 146\n['similarity', 'level']\nsimilarity 109\nlevel 63\n['between', 'words']\nbetween 14\nwords 146\n['between', 'similarity']\nbetween 14\nsimilarity 109\n['between', 'represented']\nbetween 14\nrepresented 99\n['between', 'semantic']\nbetween 14\nsemantic 101\n['words', 'represented']\nwords 146\nrepresented 99\n['words', 'between']\nwords 146\nbetween 14\n['words', 'by']\nwords 146\nby 16\n['words', 'similarity']\nwords 146\nsimilarity 109\n['represented', 'by']\nrepresented 99\nby 16\n['represented', 'words']\nrepresented 99\nwords 146\n['represented', 'those']\nrepresented 99\nthose 127\n['represented', 'between']\nrepresented 99\nbetween 14\n['by', 'those']\nby 16\nthose 127\n['by', 'represented']\nby 16\nrepresented 99\n['by', 'vectors']\nby 16\nvectors 137\n['by', 'words']\nby 16\nwords 146\n['those', 'vectors']\nthose 127\nvectors 137\n['those', 'by']\nthose 127\nby 16\n['those', 'represented']\nthose 127\nrepresented 99\n['vectors', 'those']\nvectors 137\nthose 127\n['vectors', 'by']\nvectors 137\nby 16\n['group', 'related']\ngroup 46\nrelated 96\n['group', 'models']\ngroup 46\nmodels 69\n['related', 'models']\nrelated 96\nmodels 69\n['related', 'group']\nrelated 96\ngroup 46\n['related', 'that']\nrelated 96\nthat 125\n['models', 'that']\nmodels 69\nthat 125\n['models', 'related']\nmodels 69\nrelated 96\n['models', 'used']\nmodels 69\nused 133\n['models', 'group']\nmodels 69\ngroup 46\n['that', 'used']\nthat 125\nused 133\n['that', 'models']\nthat 125\nmodels 69\n['that', 'produce']\nthat 125\nproduce 91\n['that', 'related']\nthat 125\nrelated 96\n['used', 'produce']\nused 133\nproduce 91\n['used', 'that']\nused 133\nthat 125\n['used', 'word']\nused 133\nword 145\n['used', 'models']\nused 133\nmodels 69\n['produce', 'word']\nproduce 91\nword 145\n['produce', 'used']\nproduce 91\nused 133\n['produce', 'embeddings']\nproduce 91\nembeddings 42\n['produce', 'that']\nproduce 91\nthat 125\n['word', 'embeddings']\nword 145\nembeddings 42\n['word', 'produce']\nword 145\nproduce 91\n['word', 'used']\nword 145\nused 133\n['embeddings', 'word']\nembeddings 42\nword 145\n['embeddings', 'produce']\nembeddings 42\nproduce 91\n['models', 'shallow']\nmodels 69\nshallow 105\n['models', 'twolayer']\nmodels 69\ntwolayer 130\n['shallow', 'twolayer']\nshallow 105\ntwolayer 130\n['shallow', 'models']\nshallow 105\nmodels 69\n['shallow', 'neural']\nshallow 105\nneural 76\n['twolayer', 'neural']\ntwolayer 130\nneural 76\n['twolayer', 'shallow']\ntwolayer 130\nshallow 105\n['twolayer', 'networks']\ntwolayer 130\nnetworks 75\n['twolayer', 'models']\ntwolayer 130\nmodels 69\n['neural', 'networks']\nneural 76\nnetworks 75\n['neural', 'twolayer']\nneural 76\ntwolayer 130\n['neural', 'that']\nneural 76\nthat 125\n['neural', 'shallow']\nneural 76\nshallow 105\n['networks', 'that']\nnetworks 75\nthat 125\n['networks', 'neural']\nnetworks 75\nneural 76\n['networks', 'trained']\nnetworks 75\ntrained 128\n['networks', 'twolayer']\nnetworks 75\ntwolayer 130\n['that', 'trained']\nthat 125\ntrained 128\n['that', 'networks']\nthat 125\nnetworks 75\n['that', 'reconstruct']\nthat 125\nreconstruct 95\n['that', 'neural']\nthat 125\nneural 76\n['trained', 'reconstruct']\ntrained 128\nreconstruct 95\n['trained', 'that']\ntrained 128\nthat 125\n['trained', 'linguistic']\ntrained 128\nlinguistic 64\n['trained', 'networks']\ntrained 128\nnetworks 75\n['reconstruct', 'linguistic']\nreconstruct 95\nlinguistic 64\n['reconstruct', 'trained']\nreconstruct 95\ntrained 128\n['reconstruct', 'contexts']\nreconstruct 95\ncontexts 26\n['reconstruct', 'that']\nreconstruct 95\nthat 125\n['linguistic', 'contexts']\nlinguistic 64\ncontexts 26\n['linguistic', 'reconstruct']\nlinguistic 64\nreconstruct 95\n['linguistic', 'words']\nlinguistic 64\nwords 146\n['linguistic', 'trained']\nlinguistic 64\ntrained 128\n['contexts', 'words']\ncontexts 26\nwords 146\n['contexts', 'linguistic']\ncontexts 26\nlinguistic 64\n['contexts', 'reconstruct']\ncontexts 26\nreconstruct 95\n['words', 'contexts']\nwords 146\ncontexts 26\n['words', 'linguistic']\nwords 146\nlinguistic 64\n['takes', 'input']\ntakes 120\ninput 56\n['takes', 'large']\ntakes 120\nlarge 60\n['input', 'large']\ninput 56\nlarge 60\n['input', 'takes']\ninput 56\ntakes 120\n['input', 'corpus']\ninput 56\ncorpus 29\n['large', 'corpus']\nlarge 60\ncorpus 29\n['large', 'input']\nlarge 60\ninput 56\n['large', 'text']\nlarge 60\ntext 123\n['large', 'takes']\nlarge 60\ntakes 120\n['corpus', 'text']\ncorpus 29\ntext 123\n['corpus', 'large']\ncorpus 29\nlarge 60\n['corpus', 'produces']\ncorpus 29\nproduces 92\n['corpus', 'input']\ncorpus 29\ninput 56\n['text', 'produces']\ntext 123\nproduces 92\n['text', 'corpus']\ntext 123\ncorpus 29\n['text', 'vector']\ntext 123\nvector 136\n['text', 'large']\ntext 123\nlarge 60\n['produces', 'vector']\nproduces 92\nvector 136\n['produces', 'text']\nproduces 92\ntext 123\n['produces', 'space']\nproduces 92\nspace 113\n['produces', 'corpus']\nproduces 92\ncorpus 29\n['vector', 'space']\nvector 136\nspace 113\n['vector', 'produces']\nvector 136\nproduces 92\n['vector', 'typically']\nvector 136\ntypically 131\n['vector', 'text']\nvector 136\ntext 123\n['space', 'typically']\nspace 113\ntypically 131\n['space', 'vector']\nspace 113\nvector 136\n['space', 'several']\nspace 113\nseveral 104\n['space', 'produces']\nspace 113\nproduces 92\n['typically', 'several']\ntypically 131\nseveral 104\n['typically', 'space']\ntypically 131\nspace 113\n['typically', 'hundred']\ntypically 131\nhundred 49\n['typically', 'vector']\ntypically 131\nvector 136\n['several', 'hundred']\nseveral 104\nhundred 49\n['several', 'typically']\nseveral 104\ntypically 131\n['several', 'dimensions']\nseveral 104\ndimensions 34\n['several', 'space']\nseveral 104\nspace 113\n['hundred', 'dimensions']\nhundred 49\ndimensions 34\n['hundred', 'several']\nhundred 49\nseveral 104\n['hundred', 'each']\nhundred 49\neach 39\n['hundred', 'typically']\nhundred 49\ntypically 131\n['dimensions', 'each']\ndimensions 34\neach 39\n['dimensions', 'hundred']\ndimensions 34\nhundred 49\n['dimensions', 'unique']\ndimensions 34\nunique 132\n['dimensions', 'several']\ndimensions 34\nseveral 104\n['each', 'unique']\neach 39\nunique 132\n['each', 'dimensions']\neach 39\ndimensions 34\n['each', 'word']\neach 39\nword 145\n['each', 'hundred']\neach 39\nhundred 49\n['unique', 'word']\nunique 132\nword 145\n['unique', 'each']\nunique 132\neach 39\n['unique', 'corpus']\nunique 132\ncorpus 29\n['unique', 'dimensions']\nunique 132\ndimensions 34\n['word', 'corpus']\nword 145\ncorpus 29\n['word', 'unique']\nword 145\nunique 132\n['word', 'being']\nword 145\nbeing 12\n['word', 'each']\nword 145\neach 39\n['corpus', 'being']\ncorpus 29\nbeing 12\n['corpus', 'word']\ncorpus 29\nword 145\n['corpus', 'assigned']\ncorpus 29\nassigned 7\n['corpus', 'unique']\ncorpus 29\nunique 132\n['being', 'assigned']\nbeing 12\nassigned 7\n['being', 'corpus']\nbeing 12\ncorpus 29\n['being', 'corresponding']\nbeing 12\ncorresponding 30\n['being', 'word']\nbeing 12\nword 145\n['assigned', 'corresponding']\nassigned 7\ncorresponding 30\n['assigned', 'being']\nassigned 7\nbeing 12\n['assigned', 'vector']\nassigned 7\nvector 136\n['assigned', 'corpus']\nassigned 7\ncorpus 29\n['corresponding', 'vector']\ncorresponding 30\nvector 136\n['corresponding', 'assigned']\ncorresponding 30\nassigned 7\n['corresponding', 'space']\ncorresponding 30\nspace 113\n['corresponding', 'being']\ncorresponding 30\nbeing 12\n['vector', 'space']\nvector 136\nspace 113\n['vector', 'corresponding']\nvector 136\ncorresponding 30\n['vector', 'assigned']\nvector 136\nassigned 7\n['space', 'vector']\nspace 113\nvector 136\n['space', 'corresponding']\nspace 113\ncorresponding 30\n['utilize', 'either']\nutilize 135\neither 40\n['utilize', 'two']\nutilize 135\ntwo 129\n['either', 'two']\neither 40\ntwo 129\n['either', 'utilize']\neither 40\nutilize 135\n['either', 'model']\neither 40\nmodel 68\n['two', 'model']\ntwo 129\nmodel 68\n['two', 'either']\ntwo 129\neither 40\n['two', 'architectures']\ntwo 129\narchitectures 6\n['two', 'utilize']\ntwo 129\nutilize 135\n['model', 'architectures']\nmodel 68\narchitectures 6\n['model', 'two']\nmodel 68\ntwo 129\n['model', 'produce']\nmodel 68\nproduce 91\n['model', 'either']\nmodel 68\neither 40\n['architectures', 'produce']\narchitectures 6\nproduce 91\n['architectures', 'model']\narchitectures 6\nmodel 68\n['architectures', 'distributed']\narchitectures 6\ndistributed 38\n['architectures', 'two']\narchitectures 6\ntwo 129\n['produce', 'distributed']\nproduce 91\ndistributed 38\n['produce', 'architectures']\nproduce 91\narchitectures 6\n['produce', 'representations']\nproduce 91\nrepresentations 98\n['produce', 'model']\nproduce 91\nmodel 68\n['distributed', 'representations']\ndistributed 38\nrepresentations 98\n['distributed', 'produce']\ndistributed 38\nproduce 91\n['distributed', 'words']\ndistributed 38\nwords 146\n['distributed', 'architectures']\ndistributed 38\narchitectures 6\n['representations', 'words']\nrepresentations 98\nwords 146\n['representations', 'distributed']\nrepresentations 98\ndistributed 38\n['representations', 'continuously']\nrepresentations 98\ncontinuously 28\n['representations', 'produce']\nrepresentations 98\nproduce 91\n['words', 'continuously']\nwords 146\ncontinuously 28\n['words', 'representations']\nwords 146\nrepresentations 98\n['words', 'sliding']\nwords 146\nsliding 112\n['words', 'distributed']\nwords 146\ndistributed 38\n['continuously', 'sliding']\ncontinuously 28\nsliding 112\n['continuously', 'words']\ncontinuously 28\nwords 146\n['continuously', 'bagofwords']\ncontinuously 28\nbagofwords 11\n['continuously', 'representations']\ncontinuously 28\nrepresentations 98\n['sliding', 'bagofwords']\nsliding 112\nbagofwords 11\n['sliding', 'continuously']\nsliding 112\ncontinuously 28\n['sliding', 'cbow']\nsliding 112\ncbow 20\n['sliding', 'words']\nsliding 112\nwords 146\n['bagofwords', 'cbow']\nbagofwords 11\ncbow 20\n['bagofwords', 'sliding']\nbagofwords 11\nsliding 112\n['bagofwords', 'or']\nbagofwords 11\nor 82\n['bagofwords', 'continuously']\nbagofwords 11\ncontinuously 28\n['cbow', 'or']\ncbow 20\nor 82\n['cbow', 'bagofwords']\ncbow 20\nbagofwords 11\n['cbow', 'continuously']\ncbow 20\ncontinuously 28\n['cbow', 'sliding']\ncbow 20\nsliding 112\n['or', 'continuously']\nor 82\ncontinuously 28\n['or', 'cbow']\nor 82\ncbow 20\n['or', 'sliding']\nor 82\nsliding 112\n['or', 'bagofwords']\nor 82\nbagofwords 11\n['continuously', 'sliding']\ncontinuously 28\nsliding 112\n['continuously', 'or']\ncontinuously 28\nor 82\n['continuously', 'skipgram']\ncontinuously 28\nskipgram 111\n['continuously', 'cbow']\ncontinuously 28\ncbow 20\n['sliding', 'skipgram']\nsliding 112\nskipgram 111\n['sliding', 'continuously']\nsliding 112\ncontinuously 28\n['sliding', 'or']\nsliding 112\nor 82\n['skipgram', 'sliding']\nskipgram 111\nsliding 112\n['skipgram', 'continuously']\nskipgram 111\ncontinuously 28\n['architectures', 'considers']\narchitectures 6\nconsiders 24\n['architectures', 'individual']\narchitectures 6\nindividual 52\n['considers', 'individual']\nconsiders 24\nindividual 52\n['considers', 'architectures']\nconsiders 24\narchitectures 6\n['considers', 'words']\nconsiders 24\nwords 146\n['individual', 'words']\nindividual 52\nwords 146\n['individual', 'considers']\nindividual 52\nconsiders 24\n['individual', 'sliding']\nindividual 52\nsliding 112\n['individual', 'architectures']\nindividual 52\narchitectures 6\n['words', 'sliding']\nwords 146\nsliding 112\n['words', 'individual']\nwords 146\nindividual 52\n['words', 'context']\nwords 146\ncontext 25\n['words', 'considers']\nwords 146\nconsiders 24\n['sliding', 'context']\nsliding 112\ncontext 25\n['sliding', 'words']\nsliding 112\nwords 146\n['sliding', 'window']\nsliding 112\nwindow 144\n['sliding', 'individual']\nsliding 112\nindividual 52\n['context', 'window']\ncontext 25\nwindow 144\n['context', 'sliding']\ncontext 25\nsliding 112\n['context', 'iterates']\ncontext 25\niterates 57\n['context', 'words']\ncontext 25\nwords 146\n['window', 'iterates']\nwindow 144\niterates 57\n['window', 'context']\nwindow 144\ncontext 25\n['window', 'corpus']\nwindow 144\ncorpus 29\n['window', 'sliding']\nwindow 144\nsliding 112\n['iterates', 'corpus']\niterates 57\ncorpus 29\n['iterates', 'window']\niterates 57\nwindow 144\n['iterates', 'context']\niterates 57\ncontext 25\n['corpus', 'iterates']\ncorpus 29\niterates 57\n['corpus', 'window']\ncorpus 29\nwindow 144\n['cbow', 'viewed']\ncbow 20\nviewed 138\n['cbow', '‘fill']\ncbow 20\n‘fill 147\n['viewed', '‘fill']\nviewed 138\n‘fill 147\n['viewed', 'cbow']\nviewed 138\ncbow 20\n['viewed', 'blank’']\nviewed 138\nblank’ 15\n['‘fill', 'blank’']\n‘fill 147\nblank’ 15\n['‘fill', 'viewed']\n‘fill 147\nviewed 138\n['‘fill', 'task']\n‘fill 147\ntask 121\n['‘fill', 'cbow']\n‘fill 147\ncbow 20\n['blank’', 'task']\nblank’ 15\ntask 121\n['blank’', '‘fill']\nblank’ 15\n‘fill 147\n['blank’', 'word']\nblank’ 15\nword 145\n['blank’', 'viewed']\nblank’ 15\nviewed 138\n['task', 'word']\ntask 121\nword 145\n['task', 'blank’']\ntask 121\nblank’ 15\n['task', 'embedding']\ntask 121\nembedding 41\n['task', '‘fill']\ntask 121\n‘fill 147\n['word', 'embedding']\nword 145\nembedding 41\n['word', 'task']\nword 145\ntask 121\n['word', 'represents']\nword 145\nrepresents 100\n['word', 'blank’']\nword 145\nblank’ 15\n['embedding', 'represents']\nembedding 41\nrepresents 100\n['embedding', 'word']\nembedding 41\nword 145\n['embedding', 'way']\nembedding 41\nway 139\n['embedding', 'task']\nembedding 41\ntask 121\n['represents', 'way']\nrepresents 100\nway 139\n['represents', 'embedding']\nrepresents 100\nembedding 41\n['represents', 'word']\nrepresents 100\nword 145\n['represents', 'word']\nrepresents 100\nword 145\n['way', 'word']\nway 139\nword 145\n['way', 'represents']\nway 139\nrepresents 100\n['way', 'influences']\nway 139\ninfluences 54\n['way', 'embedding']\nway 139\nembedding 41\n['word', 'influences']\nword 145\ninfluences 54\n['word', 'way']\nword 145\nway 139\n['word', 'relative']\nword 145\nrelative 97\n['word', 'represents']\nword 145\nrepresents 100\n['influences', 'relative']\ninfluences 54\nrelative 97\n['influences', 'word']\ninfluences 54\nword 145\n['influences', 'probabilities']\ninfluences 54\nprobabilities 89\n['influences', 'way']\ninfluences 54\nway 139\n['relative', 'probabilities']\nrelative 97\nprobabilities 89\n['relative', 'influences']\nrelative 97\ninfluences 54\n['relative', 'words']\nrelative 97\nwords 146\n['relative', 'word']\nrelative 97\nword 145\n['probabilities', 'words']\nprobabilities 89\nwords 146\n['probabilities', 'relative']\nprobabilities 89\nrelative 97\n['probabilities', 'context']\nprobabilities 89\ncontext 25\n['probabilities', 'influences']\nprobabilities 89\ninfluences 54\n['words', 'context']\nwords 146\ncontext 25\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "876it [00:00, 30087.87it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "['words', 'probabilities']\nwords 146\nprobabilities 89\n['words', 'window']\nwords 146\nwindow 144\n['words', 'relative']\nwords 146\nrelative 97\n['context', 'window']\ncontext 25\nwindow 144\n['context', 'words']\ncontext 25\nwords 146\n['context', 'probabilities']\ncontext 25\nprobabilities 89\n['window', 'context']\nwindow 144\ncontext 25\n['window', 'words']\nwindow 144\nwords 146\n['words', 'which']\nwords 146\nwhich 142\n['words', 'semantically']\nwords 146\nsemantically 102\n['which', 'semantically']\nwhich 142\nsemantically 102\n['which', 'words']\nwhich 142\nwords 146\n['which', 'similar']\nwhich 142\nsimilar 108\n['semantically', 'similar']\nsemantically 102\nsimilar 108\n['semantically', 'which']\nsemantically 102\nwhich 142\n['semantically', 'should']\nsemantically 102\nshould 107\n['semantically', 'words']\nsemantically 102\nwords 146\n['similar', 'should']\nsimilar 108\nshould 107\n['similar', 'semantically']\nsimilar 108\nsemantically 102\n['similar', 'influence']\nsimilar 108\ninfluence 53\n['similar', 'which']\nsimilar 108\nwhich 142\n['should', 'influence']\nshould 107\ninfluence 53\n['should', 'similar']\nshould 107\nsimilar 108\n['should', 'probabilities']\nshould 107\nprobabilities 89\n['should', 'semantically']\nshould 107\nsemantically 102\n['influence', 'probabilities']\ninfluence 53\nprobabilities 89\n['influence', 'should']\ninfluence 53\nshould 107\n['influence', 'similar']\ninfluence 53\nsimilar 108\n['influence', 'similar']\ninfluence 53\nsimilar 108\n['probabilities', 'similar']\nprobabilities 89\nsimilar 108\n['probabilities', 'influence']\nprobabilities 89\ninfluence 53\n['probabilities', 'ways']\nprobabilities 89\nways 140\n['probabilities', 'should']\nprobabilities 89\nshould 107\n['similar', 'ways']\nsimilar 108\nways 140\n['similar', 'probabilities']\nsimilar 108\nprobabilities 89\n['similar', 'semantically']\nsimilar 108\nsemantically 102\n['similar', 'influence']\nsimilar 108\ninfluence 53\n['ways', 'semantically']\nways 140\nsemantically 102\n['ways', 'similar']\nways 140\nsimilar 108\n['ways', 'similar']\nways 140\nsimilar 108\n['ways', 'probabilities']\nways 140\nprobabilities 89\n['semantically', 'similar']\nsemantically 102\nsimilar 108\n['semantically', 'ways']\nsemantically 102\nways 140\n['semantically', 'words']\nsemantically 102\nwords 146\n['semantically', 'similar']\nsemantically 102\nsimilar 108\n['similar', 'words']\nsimilar 108\nwords 146\n['similar', 'semantically']\nsimilar 108\nsemantically 102\n['similar', 'should']\nsimilar 108\nshould 107\n['similar', 'ways']\nsimilar 108\nways 140\n['words', 'should']\nwords 146\nshould 107\n['words', 'similar']\nwords 146\nsimilar 108\n['words', 'used']\nwords 146\nused 133\n['words', 'semantically']\nwords 146\nsemantically 102\n['should', 'used']\nshould 107\nused 133\n['should', 'words']\nshould 107\nwords 146\n['should', 'similar']\nshould 107\nsimilar 108\n['should', 'similar']\nshould 107\nsimilar 108\n['used', 'similar']\nused 133\nsimilar 108\n['used', 'should']\nused 133\nshould 107\n['used', 'contexts']\nused 133\ncontexts 26\n['used', 'words']\nused 133\nwords 146\n['similar', 'contexts']\nsimilar 108\ncontexts 26\n['similar', 'used']\nsimilar 108\nused 133\n['similar', 'should']\nsimilar 108\nshould 107\n['contexts', 'similar']\ncontexts 26\nsimilar 108\n['contexts', 'used']\ncontexts 26\nused 133\n['order', 'context']\norder 83\ncontext 25\n['order', 'words']\norder 83\nwords 146\n['context', 'words']\ncontext 25\nwords 146\n['context', 'order']\ncontext 25\norder 83\n['context', 'not']\ncontext 25\nnot 78\n['words', 'not']\nwords 146\nnot 78\n['words', 'context']\nwords 146\ncontext 25\n['words', 'influence']\nwords 146\ninfluence 53\n['words', 'order']\nwords 146\norder 83\n['not', 'influence']\nnot 78\ninfluence 53\n['not', 'words']\nnot 78\nwords 146\n['not', 'prediction']\nnot 78\nprediction 88\n['not', 'context']\nnot 78\ncontext 25\n['influence', 'prediction']\ninfluence 53\nprediction 88\n['influence', 'not']\ninfluence 53\nnot 78\n['influence', 'bagofwords']\ninfluence 53\nbagofwords 11\n['influence', 'words']\ninfluence 53\nwords 146\n['prediction', 'bagofwords']\nprediction 88\nbagofwords 11\n['prediction', 'influence']\nprediction 88\ninfluence 53\n['prediction', 'assumption']\nprediction 88\nassumption 9\n['prediction', 'not']\nprediction 88\nnot 78\n['bagofwords', 'assumption']\nbagofwords 11\nassumption 9\n['bagofwords', 'prediction']\nbagofwords 11\nprediction 88\n['bagofwords', 'influence']\nbagofwords 11\ninfluence 53\n['assumption', 'bagofwords']\nassumption 9\nbagofwords 11\n['assumption', 'prediction']\nassumption 9\nprediction 88\n['continuous', 'skipgram']\ncontinuous 27\nskipgram 111\n['continuous', 'architecture']\ncontinuous 27\narchitecture 5\n['skipgram', 'architecture']\nskipgram 111\narchitecture 5\n['skipgram', 'continuous']\nskipgram 111\ncontinuous 27\n['skipgram', 'model']\nskipgram 111\nmodel 68\n['architecture', 'model']\narchitecture 5\nmodel 68\n['architecture', 'skipgram']\narchitecture 5\nskipgram 111\n['architecture', 'uses']\narchitecture 5\nuses 134\n['architecture', 'continuous']\narchitecture 5\ncontinuous 27\n['model', 'uses']\nmodel 68\nuses 134\n['model', 'architecture']\nmodel 68\narchitecture 5\n['model', 'current']\nmodel 68\ncurrent 32\n['model', 'skipgram']\nmodel 68\nskipgram 111\n['uses', 'current']\nuses 134\ncurrent 32\n['uses', 'model']\nuses 134\nmodel 68\n['uses', 'word']\nuses 134\nword 145\n['uses', 'architecture']\nuses 134\narchitecture 5\n['current', 'word']\ncurrent 32\nword 145\n['current', 'uses']\ncurrent 32\nuses 134\n['current', 'predict']\ncurrent 32\npredict 87\n['current', 'model']\ncurrent 32\nmodel 68\n['word', 'predict']\nword 145\npredict 87\n['word', 'current']\nword 145\ncurrent 32\n['word', 'surrounding']\nword 145\nsurrounding 116\n['word', 'uses']\nword 145\nuses 134\n['predict', 'surrounding']\npredict 87\nsurrounding 116\n['predict', 'word']\npredict 87\nword 145\n['predict', 'window']\npredict 87\nwindow 144\n['predict', 'current']\npredict 87\ncurrent 32\n['surrounding', 'window']\nsurrounding 116\nwindow 144\n['surrounding', 'predict']\nsurrounding 116\npredict 87\n['surrounding', 'context']\nsurrounding 116\ncontext 25\n['surrounding', 'word']\nsurrounding 116\nword 145\n['window', 'context']\nwindow 144\ncontext 25\n['window', 'surrounding']\nwindow 144\nsurrounding 116\n['window', 'words']\nwindow 144\nwords 146\n['window', 'predict']\nwindow 144\npredict 87\n['context', 'words']\ncontext 25\nwords 146\n['context', 'window']\ncontext 25\nwindow 144\n['context', 'surrounding']\ncontext 25\nsurrounding 116\n['words', 'context']\nwords 146\ncontext 25\n['words', 'window']\nwords 146\nwindow 144\n['skipgram', 'architecture']\nskipgram 111\narchitecture 5\n['skipgram', 'weighs']\nskipgram 111\nweighs 141\n['architecture', 'weighs']\narchitecture 5\nweighs 141\n['architecture', 'skipgram']\narchitecture 5\nskipgram 111\n['architecture', 'nearby']\narchitecture 5\nnearby 73\n['weighs', 'nearby']\nweighs 141\nnearby 73\n['weighs', 'architecture']\nweighs 141\narchitecture 5\n['weighs', 'context']\nweighs 141\ncontext 25\n['weighs', 'skipgram']\nweighs 141\nskipgram 111\n['nearby', 'context']\nnearby 73\ncontext 25\n['nearby', 'weighs']\nnearby 73\nweighs 141\n['nearby', 'words']\nnearby 73\nwords 146\n['nearby', 'architecture']\nnearby 73\narchitecture 5\n['context', 'words']\ncontext 25\nwords 146\n['context', 'nearby']\ncontext 25\nnearby 73\n['context', 'more']\ncontext 25\nmore 70\n['context', 'weighs']\ncontext 25\nweighs 141\n['words', 'more']\nwords 146\nmore 70\n['words', 'context']\nwords 146\ncontext 25\n['words', 'heavily']\nwords 146\nheavily 48\n['words', 'nearby']\nwords 146\nnearby 73\n['more', 'heavily']\nmore 70\nheavily 48\n['more', 'words']\nmore 70\nwords 146\n['more', 'than']\nmore 70\nthan 124\n['more', 'context']\nmore 70\ncontext 25\n['heavily', 'than']\nheavily 48\nthan 124\n['heavily', 'more']\nheavily 48\nmore 70\n['heavily', 'more']\nheavily 48\nmore 70\n['heavily', 'words']\nheavily 48\nwords 146\n['than', 'more']\nthan 124\nmore 70\n['than', 'heavily']\nthan 124\nheavily 48\n['than', 'distant']\nthan 124\ndistant 36\n['than', 'more']\nthan 124\nmore 70\n['more', 'distant']\nmore 70\ndistant 36\n['more', 'than']\nmore 70\nthan 124\n['more', 'context']\nmore 70\ncontext 25\n['more', 'heavily']\nmore 70\nheavily 48\n['distant', 'context']\ndistant 36\ncontext 25\n['distant', 'more']\ndistant 36\nmore 70\n['distant', 'words']\ndistant 36\nwords 146\n['distant', 'than']\ndistant 36\nthan 124\n['context', 'words']\ncontext 25\nwords 146\n['context', 'distant']\ncontext 25\ndistant 36\n['context', 'more']\ncontext 25\nmore 70\n['words', 'context']\nwords 146\ncontext 25\n['words', 'distant']\nwords 146\ndistant 36\n['according', 'authors']\naccording 0\nauthors 10\n['according', 'cbow']\naccording 0\ncbow 20\n['authors', 'cbow']\nauthors 10\ncbow 20\n['authors', 'according']\nauthors 10\naccording 0\n['authors', 'faster']\nauthors 10\nfaster 44\n['cbow', 'faster']\ncbow 20\nfaster 44\n['cbow', 'authors']\ncbow 20\nauthors 10\n['cbow', 'while']\ncbow 20\nwhile 143\n['cbow', 'according']\ncbow 20\naccording 0\n['faster', 'while']\nfaster 44\nwhile 143\n['faster', 'cbow']\nfaster 44\ncbow 20\n['faster', 'skipgram']\nfaster 44\nskipgram 111\n['faster', 'authors']\nfaster 44\nauthors 10\n['while', 'skipgram']\nwhile 143\nskipgram 111\n['while', 'faster']\nwhile 143\nfaster 44\n['while', 'better']\nwhile 143\nbetter 13\n['while', 'cbow']\nwhile 143\ncbow 20\n['skipgram', 'better']\nskipgram 111\nbetter 13\n['skipgram', 'while']\nskipgram 111\nwhile 143\n['skipgram', 'job']\nskipgram 111\njob 58\n['skipgram', 'faster']\nskipgram 111\nfaster 44\n['better', 'job']\nbetter 13\njob 58\n['better', 'skipgram']\nbetter 13\nskipgram 111\n['better', 'infrequent']\nbetter 13\ninfrequent 55\n['better', 'while']\nbetter 13\nwhile 143\n['job', 'infrequent']\njob 58\ninfrequent 55\n['job', 'better']\njob 58\nbetter 13\n['job', 'words']\njob 58\nwords 146\n['job', 'skipgram']\njob 58\nskipgram 111\n['infrequent', 'words']\ninfrequent 55\nwords 146\n['infrequent', 'job']\ninfrequent 55\njob 58\n['infrequent', 'better']\ninfrequent 55\nbetter 13\n['words', 'infrequent']\nwords 146\ninfrequent 55\n['words', 'job']\nwords 146\njob 58\n['after', 'model']\nafter 2\nmodel 68\n['after', 'has']\nafter 2\nhas 47\n['model', 'has']\nmodel 68\nhas 47\n['model', 'after']\nmodel 68\nafter 2\n['model', 'trained']\nmodel 68\ntrained 128\n['has', 'trained']\nhas 47\ntrained 128\n['has', 'model']\nhas 47\nmodel 68\n['has', 'learned']\nhas 47\nlearned 62\n['has', 'after']\nhas 47\nafter 2\n['trained', 'learned']\ntrained 128\nlearned 62\n['trained', 'has']\ntrained 128\nhas 47\n['trained', 'word']\ntrained 128\nword 145\n['trained', 'model']\ntrained 128\nmodel 68\n['learned', 'word']\nlearned 62\nword 145\n['learned', 'trained']\nlearned 62\ntrained 128\n['learned', 'embeddings']\nlearned 62\nembeddings 42\n['learned', 'has']\nlearned 62\nhas 47\n['word', 'embeddings']\nword 145\nembeddings 42\n['word', 'learned']\nword 145\nlearned 62\n['word', 'positioned']\nword 145\npositioned 86\n['word', 'trained']\nword 145\ntrained 128\n['embeddings', 'positioned']\nembeddings 42\npositioned 86\n['embeddings', 'word']\nembeddings 42\nword 145\n['embeddings', 'vector']\nembeddings 42\nvector 136\n['embeddings', 'learned']\nembeddings 42\nlearned 62\n['positioned', 'vector']\npositioned 86\nvector 136\n['positioned', 'embeddings']\npositioned 86\nembeddings 42\n['positioned', 'space']\npositioned 86\nspace 113\n['positioned', 'word']\npositioned 86\nword 145\n['vector', 'space']\nvector 136\nspace 113\n['vector', 'positioned']\nvector 136\npositioned 86\n['vector', 'such']\nvector 136\nsuch 114\n['vector', 'embeddings']\nvector 136\nembeddings 42\n['space', 'such']\nspace 113\nsuch 114\n['space', 'vector']\nspace 113\nvector 136\n['space', 'that']\nspace 113\nthat 125\n['space', 'positioned']\nspace 113\npositioned 86\n['such', 'that']\nsuch 114\nthat 125\n['such', 'space']\nsuch 114\nspace 113\n['such', 'words']\nsuch 114\nwords 146\n['such', 'vector']\nsuch 114\nvector 136\n['that', 'words']\nthat 125\nwords 146\n['that', 'such']\nthat 125\nsuch 114\n['that', 'that']\nthat 125\nthat 125\n['that', 'space']\nthat 125\nspace 113\n['words', 'that']\nwords 146\nthat 125\n['words', 'that']\nwords 146\nthat 125\n['words', 'share']\nwords 146\nshare 106\n['words', 'such']\nwords 146\nsuch 114\n['that', 'share']\nthat 125\nshare 106\n['that', 'words']\nthat 125\nwords 146\n['that', 'common']\nthat 125\ncommon 23\n['that', 'that']\nthat 125\nthat 125\n['share', 'common']\nshare 106\ncommon 23\n['share', 'that']\nshare 106\nthat 125\n['share', 'contexts']\nshare 106\ncontexts 26\n['share', 'words']\nshare 106\nwords 146\n['common', 'contexts']\ncommon 23\ncontexts 26\n['common', 'share']\ncommon 23\nshare 106\n['common', 'corpus']\ncommon 23\ncorpus 29\n['common', 'that']\ncommon 23\nthat 125\n['contexts', 'corpus']\ncontexts 26\ncorpus 29\n['contexts', 'common']\ncontexts 26\ncommon 23\n['contexts', 'that']\ncontexts 26\nthat 125\n['contexts', 'share']\ncontexts 26\nshare 106\n['corpus', 'that']\ncorpus 29\nthat 125\n['corpus', 'contexts']\ncorpus 29\ncontexts 26\n['corpus', 'words']\ncorpus 29\nwords 146\n['corpus', 'common']\ncorpus 29\ncommon 23\n['that', 'words']\nthat 125\nwords 146\n['that', 'corpus']\nthat 125\ncorpus 29\n['that', 'that']\nthat 125\nthat 125\n['that', 'contexts']\nthat 125\ncontexts 26\n['words', 'that']\nwords 146\nthat 125\n['words', 'that']\nwords 146\nthat 125\n['words', 'semantically']\nwords 146\nsemantically 102\n['words', 'corpus']\nwords 146\ncorpus 29\n['that', 'semantically']\nthat 125\nsemantically 102\n['that', 'words']\nthat 125\nwords 146\n['that', 'syntactically']\nthat 125\nsyntactically 119\n['that', 'that']\nthat 125\nthat 125\n['semantically', 'syntactically']\nsemantically 102\nsyntactically 119\n['semantically', 'that']\nsemantically 102\nthat 125\n['semantically', 'similar']\nsemantically 102\nsimilar 108\n['semantically', 'words']\nsemantically 102\nwords 146\n['syntactically', 'similar']\nsyntactically 119\nsimilar 108\n['syntactically', 'semantically']\nsyntactically 119\nsemantically 102\n['syntactically', 'located']\nsyntactically 119\nlocated 66\n['syntactically', 'that']\nsyntactically 119\nthat 125\n['similar', 'located']\nsimilar 108\nlocated 66\n['similar', 'syntactically']\nsimilar 108\nsyntactically 119\n['similar', 'close']\nsimilar 108\nclose 22\n['similar', 'semantically']\nsimilar 108\nsemantically 102\n['located', 'close']\nlocated 66\nclose 22\n['located', 'similar']\nlocated 66\nsimilar 108\n['located', 'one']\nlocated 66\none 81\n['located', 'syntactically']\nlocated 66\nsyntactically 119\n['close', 'one']\nclose 22\none 81\n['close', 'located']\nclose 22\nlocated 66\n['close', 'another']\nclose 22\nanother 4\n['close', 'similar']\nclose 22\nsimilar 108\n['one', 'another']\none 81\nanother 4\n['one', 'close']\none 81\nclose 22\n['one', 'space']\none 81\nspace 113\n['one', 'located']\none 81\nlocated 66\n['another', 'space']\nanother 4\nspace 113\n['another', 'one']\nanother 4\none 81\n['another', 'close']\nanother 4\nclose 22\n['space', 'another']\nspace 113\nanother 4\n['space', 'one']\nspace 113\none 81\n['more', 'dissimilar']\nmore 70\ndissimilar 35\n['more', 'words']\nmore 70\nwords 146\n['dissimilar', 'words']\ndissimilar 35\nwords 146\n['dissimilar', 'more']\ndissimilar 35\nmore 70\n['dissimilar', 'located']\ndissimilar 35\nlocated 66\n['words', 'located']\nwords 146\nlocated 66\n['words', 'dissimilar']\nwords 146\ndissimilar 35\n['words', 'farther']\nwords 146\nfarther 43\n['words', 'more']\nwords 146\nmore 70\n['located', 'farther']\nlocated 66\nfarther 43\n['located', 'words']\nlocated 66\nwords 146\n['located', 'one']\nlocated 66\none 81\n['located', 'dissimilar']\nlocated 66\ndissimilar 35\n['farther', 'one']\nfarther 43\none 81\n['farther', 'located']\nfarther 43\nlocated 66\n['farther', 'another']\nfarther 43\nanother 4\n['farther', 'words']\nfarther 43\nwords 146\n['one', 'another']\none 81\nanother 4\n['one', 'farther']\none 81\nfarther 43\n['one', 'space']\none 81\nspace 113\n['one', 'located']\none 81\nlocated 66\n['another', 'space']\nanother 4\nspace 113\n['another', 'one']\nanother 4\none 81\n['another', 'farther']\nanother 4\nfarther 43\n['space', 'another']\nspace 113\nanother 4\n['space', 'one']\nspace 113\none 81\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": "matrix_x1 = tf.convert_to_tensor(matrix_x1, dtype=tf.float32)\nmatrix_y1 = tf.convert_to_tensor(matrix_y1, dtype=tf.float32)\nprint(matrix_x1.shape)\nprint(matrix_y1.shape)",
   "metadata": {
    "papermill": {
     "duration": 0.120392,
     "end_time": "2023-12-01T13:37:44.008233",
     "exception": false,
     "start_time": "2023-12-01T13:37:43.887841",
     "status": "completed"
    },
    "tags": [],
    "id": "b26e63c7",
    "outputId": "96d6e790-508a-48d4-f2ad-37c9f810e220",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:27:00.469020300Z",
     "start_time": "2023-12-03T05:26:59.794689800Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:37.321421Z",
     "iopub.execute_input": "2025-07-05T17:37:37.321606Z",
     "iopub.status.idle": "2025-07-05T17:37:38.443965Z",
     "shell.execute_reply.started": "2025-07-05T17:37:37.321593Z",
     "shell.execute_reply": "2025-07-05T17:37:38.443241Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "I0000 00:00:1751737058.392503      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "(876, 148)\n(876, 148)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": [
    "def CreateModel(input, output):\n",
    "    # Defining the size of the embedding\n",
    "    embed_size = 2\n",
    "    # Defining the neural network\n",
    "\n",
    "    inp = Input(shape=input)  # 21\n",
    "    x = Dense(units=embed_size, activation=\"linear\")(inp)\n",
    "    x = Dense(units=output, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=\"accuracy\")\n",
    "    model.summary()\n",
    "    return model"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.02323,
     "end_time": "2023-12-01T13:37:44.042461",
     "exception": false,
     "start_time": "2023-12-01T13:37:44.019231",
     "status": "completed"
    },
    "tags": [],
    "id": "0148425d",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:27:00.469020300Z",
     "start_time": "2023-12-03T05:26:59.823215500Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.444786Z",
     "iopub.execute_input": "2025-07-05T17:37:38.445015Z",
     "iopub.status.idle": "2025-07-05T17:37:38.449777Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.444999Z",
     "shell.execute_reply": "2025-07-05T17:37:38.44899Z"
    }
   },
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": "model1 = CreateModel(matrix_x1.shape[1], matrix_y1.shape[1])",
   "metadata": {
    "id": "VOC2RLk_7p-g",
    "outputId": "e95b6856-0d88-4356-aa39-551a257455f3",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:27:00.552020800Z",
     "start_time": "2023-12-03T05:26:59.829393200Z"
    },
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model1.fit(x=matrix_x1, y=matrix_y1, epochs=2000)"
   ],
   "metadata": {
    "papermill": {
     "duration": 49.28302,
     "end_time": "2023-12-01T13:38:33.336416",
     "exception": false,
     "start_time": "2023-12-01T13:37:44.053396",
     "status": "completed"
    },
    "tags": [],
    "id": "59cda6e4",
    "outputId": "e5927ce9-c864-40b2-ebf1-e0e0a8fa9bd5",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:30:00.452539500Z",
     "start_time": "2023-12-03T05:27:00.006294Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.763069Z",
     "iopub.status.idle": "2025-07-05T17:37:38.763269Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.763173Z",
     "shell.execute_reply": "2025-07-05T17:37:38.763182Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(model1.history.history[\"loss\"])\n",
    "plt.plot(model1.history.history[\"accuracy\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "40bef5b618bff8c5",
    "outputId": "869fd24d-3e00-4a9d-d6ca-080e269d3a7d",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:30:01.289428500Z",
     "start_time": "2023-12-03T05:29:59.621858800Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.764139Z",
     "iopub.status.idle": "2025-07-05T17:37:38.764469Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.764296Z",
     "shell.execute_reply": "2025-07-05T17:37:38.76431Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# The input layer\n\nweights1 = model1.get_weights()[0]\nprint(weights1.shape)\n# print(weights[1][1])\n# print(weights)\n\n# weights = model.get_weights()[2]\n# print (weights)",
   "metadata": {
    "papermill": {
     "duration": 0.620854,
     "end_time": "2023-12-01T13:38:34.535985",
     "exception": false,
     "start_time": "2023-12-01T13:38:33.915131",
     "status": "completed"
    },
    "tags": [],
    "id": "4268a6b8",
    "outputId": "b549f4a2-1785-4772-eb45-529ef26488b7",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:30:01.357704800Z",
     "start_time": "2023-12-03T05:30:00.465679400Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.765243Z",
     "iopub.status.idle": "2025-07-05T17:37:38.765455Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.76535Z",
     "shell.execute_reply": "2025-07-05T17:37:38.765358Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def create_embedding_word_dict(unique_word, unique_word_dict, weights):\n",
    "    # get the weight for each unique word\n",
    "    embedding_dict = {}\n",
    "    for (\n",
    "        word\n",
    "    ) in (\n",
    "        unique_word\n",
    "    ):  # to pick a row of weight of two values for each unique word since weights = 21*2\n",
    "        embedding_dict.update({word: weights[unique_word_dict.get(word)]})\n",
    "    return embedding_dict"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.651547,
     "end_time": "2023-12-01T13:38:35.77262",
     "exception": false,
     "start_time": "2023-12-01T13:38:35.121073",
     "status": "completed"
    },
    "tags": [],
    "id": "70ba0840",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:30:01.358702700Z",
     "start_time": "2023-12-03T05:30:00.501497800Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.766378Z",
     "iopub.status.idle": "2025-07-05T17:37:38.766693Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.766519Z",
     "shell.execute_reply": "2025-07-05T17:37:38.766535Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "embedding_dict1 = create_embedding_word_dict(unique_word1, unique_word_dict1, weights1)",
   "metadata": {
    "papermill": {
     "duration": 0.592325,
     "end_time": "2023-12-01T13:38:36.943729",
     "exception": false,
     "start_time": "2023-12-01T13:38:36.351404",
     "status": "completed"
    },
    "tags": [],
    "id": "7b8e3f5c",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:30:01.358702700Z",
     "start_time": "2023-12-03T05:30:00.510918600Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.767451Z",
     "iopub.status.idle": "2025-07-05T17:37:38.767775Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.767606Z",
     "shell.execute_reply": "2025-07-05T17:37:38.76762Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "embedding_dict1",
   "metadata": {
    "id": "_czuprCm_9aL",
    "outputId": "18fc00e6-ac0d-4225-e207-4493d73bea5c",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:30:01.422761900Z",
     "start_time": "2023-12-03T05:30:00.524404500Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.768779Z",
     "iopub.status.idle": "2025-07-05T17:37:38.769068Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.768921Z",
     "shell.execute_reply": "2025-07-05T17:37:38.768935Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# fig, axs = plt.subplots(len(embedding_dict.keys()), 3, figsize=(15, 10), subplot_kw={'projection': '3d'})\n",
    "# print(len(embedding_dict.keys()))\n",
    "# axs = axs.flatten()\n",
    "# # Iterate through unique_word and plot in each subplot\n",
    "# for i, word in enumerate(unique_word):\n",
    "#     coord = embedding_dict.get(word)\n",
    "#     ax = axs[i]\n",
    "\n",
    "#     if coord is not None:\n",
    "#         ax.scatter(coord[0], coord[1], coord[2], c='b', marker='o')\n",
    "# ax.text(coord[0], coord[1], coord[2], s=word)\n",
    "# ax.set_xlabel('X')\n",
    "# ax.set_ylabel('Y')\n",
    "# ax.set_zlabel('Z')\n",
    "# ax.set_title(word)\n",
    "\n",
    "# Remove empty subplots\n",
    "# for j in range(len(unique_word), len(axs)):\n",
    "#     fig.delaxes(axs[j])\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# i = 0\n",
    "# ax_list = []\n",
    "# for word in unique_word:\n",
    "#     #print(i, ' >> ', word)\n",
    "#     coord = embedding_dict.get(word)\n",
    "#     fig = plt.figure()\n",
    "#     ax = fig.add_subplot( projection='3d')\n",
    "#     ax.scatter(coord[0], coord[1], coord[2], c='b', marker='o')\n",
    "#     ax.text(coord[0], coord[1], coord[2], s=word)\n",
    "#     ax.set_xlabel('X')\n",
    "#     ax.set_ylabel('Y')\n",
    "#     ax.set_zlabel('Z')\n",
    "#     ax.legend()\n",
    "#     ax_list.append(ax)\n",
    "\n",
    "#     # plt.scatter(coord[0], coord[1])\n",
    "#     # plt.annotate(word, (coord[0], coord[1]))\n",
    "#     i = i + 1\n",
    "#     plt.legend(unique_word)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for word in unique_word1:\n",
    "    coord = embedding_dict1.get(word)\n",
    "    plt.scatter(coord[0], coord[1])\n",
    "    plt.annotate(word, (coord[0], coord[1]))"
   ],
   "metadata": {
    "papermill": {
     "duration": 1.412124,
     "end_time": "2023-12-01T13:38:38.938513",
     "exception": false,
     "start_time": "2023-12-01T13:38:37.526389",
     "status": "completed"
    },
    "tags": [],
    "id": "a07a3e3d",
    "outputId": "6b3c4093-543e-458d-c8be-7fe4c2b79e78",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:30:09.792543200Z",
     "start_time": "2023-12-03T05:30:00.588168900Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.769909Z",
     "iopub.status.idle": "2025-07-05T17:37:38.770195Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.770049Z",
     "shell.execute_reply": "2025-07-05T17:37:38.770063Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "plt.figure(figsize=(10, 10))\ni = 0\nweights1 = model1.get_weights()[0]\nfor word in list(unique_word_dict1.keys()):\n    coord = embedding_dict1.get(word)\n    if weights1[i][0] < 0 < weights1[i][1]:\n        plt.scatter(0, weights1[i][1])\n        plt.annotate(word, (0, weights1[i][1]))\n    else:\n        plt.scatter(weights1[i][0], weights1[i][1])\n        plt.annotate(word, (weights1[i][0], weights1[i][1]))",
   "metadata": {
    "papermill": {
     "duration": 1.310399,
     "end_time": "2023-12-01T13:38:41.989224",
     "exception": false,
     "start_time": "2023-12-01T13:38:40.678825",
     "status": "completed"
    },
    "tags": [],
    "id": "812c32a8",
    "ExecuteTime": {
     "end_time": "2023-12-03T05:30:14.538838900Z",
     "start_time": "2023-12-03T05:30:09.791543400Z"
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.771369Z",
     "iopub.status.idle": "2025-07-05T17:37:38.771623Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.77151Z",
     "shell.execute_reply": "2025-07-05T17:37:38.771523Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "texts2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:30:14.600297900Z",
     "start_time": "2023-12-03T05:30:14.539845800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.772282Z",
     "iopub.status.idle": "2025-07-05T17:37:38.772547Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.772402Z",
     "shell.execute_reply": "2025-07-05T17:37:38.772412Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "word_lists2, all_text2 = get_training_data(texts2)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:30:14.829827900Z",
     "start_time": "2023-12-03T05:30:14.549374800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.773794Z",
     "iopub.status.idle": "2025-07-05T17:37:38.774052Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.773939Z",
     "shell.execute_reply": "2025-07-05T17:37:38.773951Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "word_lists2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:30:14.831826600Z",
     "start_time": "2023-12-03T05:30:14.564725500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.774816Z",
     "iopub.status.idle": "2025-07-05T17:37:38.775112Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.774959Z",
     "shell.execute_reply": "2025-07-05T17:37:38.774972Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "all_text2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:30:14.907046800Z",
     "start_time": "2023-12-03T05:30:14.596302600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.775906Z",
     "iopub.status.idle": "2025-07-05T17:37:38.776184Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.776025Z",
     "shell.execute_reply": "2025-07-05T17:37:38.776038Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "unique_word_dict2, number_of_unique_words2 = create_unique_word_dict(all_text2)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:30:14.907046800Z",
     "start_time": "2023-12-03T05:30:14.606845100Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.776939Z",
     "iopub.status.idle": "2025-07-05T17:37:38.777151Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.777053Z",
     "shell.execute_reply": "2025-07-05T17:37:38.777063Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "unique_word_dict2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:30:14.908047600Z",
     "start_time": "2023-12-03T05:30:14.617858700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.77802Z",
     "iopub.status.idle": "2025-07-05T17:37:38.778229Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.778124Z",
     "shell.execute_reply": "2025-07-05T17:37:38.778132Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "number_of_unique_words2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:30:14.909044400Z",
     "start_time": "2023-12-03T05:30:14.629719600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.779331Z",
     "iopub.status.idle": "2025-07-05T17:37:38.779631Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.779484Z",
     "shell.execute_reply": "2025-07-05T17:37:38.779497Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "unique_word2 = list(unique_word_dict2.keys())",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:30:14.979591800Z",
     "start_time": "2023-12-03T05:30:14.649496200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.780945Z",
     "iopub.status.idle": "2025-07-05T17:37:38.781234Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.781064Z",
     "shell.execute_reply": "2025-07-05T17:37:38.781075Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "unique_word2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:30:14.980583400Z",
     "start_time": "2023-12-03T05:30:14.657246400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.782269Z",
     "iopub.status.idle": "2025-07-05T17:37:38.782514Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.782384Z",
     "shell.execute_reply": "2025-07-05T17:37:38.782394Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "matrix_x2, matrix_y2 = one_hot_representation(\n",
    "    word_lists2, number_of_unique_words2, unique_word_dict2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:30:14.983586700Z",
     "start_time": "2023-12-03T05:30:14.669122500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.783523Z",
     "iopub.status.idle": "2025-07-05T17:37:38.783789Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.783657Z",
     "shell.execute_reply": "2025-07-05T17:37:38.783688Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "matrix_x2 = tf.convert_to_tensor(matrix_x2, dtype=tf.float32)\nmatrix_y2 = tf.convert_to_tensor(matrix_y2, dtype=tf.float32)\nprint(matrix_x2.shape)\nprint(matrix_y2.shape)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:30:15.005583300Z",
     "start_time": "2023-12-03T05:30:14.710063100Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.784645Z",
     "iopub.status.idle": "2025-07-05T17:37:38.784921Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.78481Z",
     "shell.execute_reply": "2025-07-05T17:37:38.784823Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "model2 = CreateModel(matrix_x2.shape[1], matrix_y2.shape[1])",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:30:15.027585900Z",
     "start_time": "2023-12-03T05:30:14.717265800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.785944Z",
     "iopub.status.idle": "2025-07-05T17:37:38.786234Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.78609Z",
     "shell.execute_reply": "2025-07-05T17:37:38.786103Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model2.fit(x=matrix_x2, y=matrix_y2, epochs=2000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:32:12.703494200Z",
     "start_time": "2023-12-03T05:30:14.801829500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.787109Z",
     "iopub.status.idle": "2025-07-05T17:37:38.787413Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.787261Z",
     "shell.execute_reply": "2025-07-05T17:37:38.787276Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(model2.history.history[\"loss\"])\n",
    "plt.plot(model2.history.history[\"accuracy\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:32:12.878419300Z",
     "start_time": "2023-12-03T05:31:59.321810100Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.78896Z",
     "iopub.status.idle": "2025-07-05T17:37:38.789206Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.789084Z",
     "shell.execute_reply": "2025-07-05T17:37:38.789093Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "weights2 = model2.get_weights()[0]",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:32:12.879416800Z",
     "start_time": "2023-12-03T05:31:59.746220300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.790319Z",
     "iopub.status.idle": "2025-07-05T17:37:38.79052Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.790426Z",
     "shell.execute_reply": "2025-07-05T17:37:38.790435Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "weights2.shape",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:32:12.880417100Z",
     "start_time": "2023-12-03T05:31:59.757754700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.791535Z",
     "iopub.status.idle": "2025-07-05T17:37:38.791851Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.791695Z",
     "shell.execute_reply": "2025-07-05T17:37:38.791707Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "embedding_dict2 = create_embedding_word_dict(unique_word2, unique_word_dict2, weights2)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:32:12.880417100Z",
     "start_time": "2023-12-03T05:31:59.767873800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.792697Z",
     "iopub.status.idle": "2025-07-05T17:37:38.792949Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.79283Z",
     "shell.execute_reply": "2025-07-05T17:37:38.792843Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "embedding_dict2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:32:12.881415700Z",
     "start_time": "2023-12-03T05:31:59.775940500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.793728Z",
     "iopub.status.idle": "2025-07-05T17:37:38.793976Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.793867Z",
     "shell.execute_reply": "2025-07-05T17:37:38.793879Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "plt.figure(figsize=(10, 10))\nfor word in unique_word2:\n    coord = embedding_dict2.get(word)\n    plt.scatter(coord[0], coord[1])\n    plt.annotate(word, (coord[0], coord[1]))",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:32:14.503464500Z",
     "start_time": "2023-12-03T05:31:59.815434700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.794786Z",
     "iopub.status.idle": "2025-07-05T17:37:38.795057Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.7949Z",
     "shell.execute_reply": "2025-07-05T17:37:38.794913Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "plt.figure(figsize=(10, 10))\ni = 0\nfor word in list(unique_word_dict2.keys()):\n    coord = embedding_dict2.get(word)\n    if weights2[i][0] < 0 < weights2[i][1]:\n        plt.scatter(0, weights2[i][1])\n        plt.annotate(word, (0, weights2[i][1]))\n    else:\n        plt.scatter(weights2[i][0], weights2[i][1])\n        plt.annotate(word, (weights2[i][0], weights2[i][1]))",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:32:17.083071900Z",
     "start_time": "2023-12-03T05:32:03.483757900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.796254Z",
     "iopub.status.idle": "2025-07-05T17:37:38.796553Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.796393Z",
     "shell.execute_reply": "2025-07-05T17:37:38.796406Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "embedding1 = list(embedding_dict1.values())\nembedding2 = list(embedding_dict2.values())",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T06:12:55.639043500Z",
     "start_time": "2023-12-03T06:12:55.240875200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.79732Z",
     "iopub.status.idle": "2025-07-05T17:37:38.797513Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.797419Z",
     "shell.execute_reply": "2025-07-05T17:37:38.797429Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "cosineSimilarity = cosine_similarity(embedding1, embedding2)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:32:17.084108800Z",
     "start_time": "2023-12-03T05:32:07.389524100Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.798395Z",
     "iopub.status.idle": "2025-07-05T17:37:38.798714Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.798547Z",
     "shell.execute_reply": "2025-07-05T17:37:38.79856Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "cosineSimilarity",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:32:17.085070400Z",
     "start_time": "2023-12-03T05:32:07.410193400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.799522Z",
     "iopub.status.idle": "2025-07-05T17:37:38.799826Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.799679Z",
     "shell.execute_reply": "2025-07-05T17:37:38.799693Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "sns.heatmap(cosineSimilarity, cmap=\"YlGnBu\")",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:32:17.846648Z",
     "start_time": "2023-12-03T05:32:07.420362100Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.801299Z",
     "iopub.status.idle": "2025-07-05T17:37:38.80161Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.801456Z",
     "shell.execute_reply": "2025-07-05T17:37:38.801469Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "euclideanDistances = euclidean_distances(embedding1, embedding2)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:32:17.846648Z",
     "start_time": "2023-12-03T05:32:08.676099200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.802783Z",
     "iopub.status.idle": "2025-07-05T17:37:38.803092Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.802933Z",
     "shell.execute_reply": "2025-07-05T17:37:38.802948Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "euclideanDistances",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:32:17.846648Z",
     "start_time": "2023-12-03T05:32:08.691630300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.804203Z",
     "iopub.status.idle": "2025-07-05T17:37:38.80443Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.804316Z",
     "shell.execute_reply": "2025-07-05T17:37:38.804324Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "sns.heatmap(euclideanDistances, cmap=\"YlGnBu\")",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:32:18.740631900Z",
     "start_time": "2023-12-03T05:32:08.705855900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.805588Z",
     "iopub.status.idle": "2025-07-05T17:37:38.805819Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.805718Z",
     "shell.execute_reply": "2025-07-05T17:37:38.805728Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "from sklearn.manifold import MDS",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.806875Z",
     "iopub.status.idle": "2025-07-05T17:37:38.807168Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.807027Z",
     "shell.execute_reply": "2025-07-05T17:37:38.807047Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "embedding1 = np.array(embedding1)\nembedding2 = np.array(embedding2)\nembedding = np.concatenate((embedding1, embedding2), axis=0)\nembedding_s = np.cov(embedding)\nembedding_s.shape",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T06:13:01.294074900Z",
     "start_time": "2023-12-03T06:13:01.140695Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.808171Z",
     "iopub.status.idle": "2025-07-05T17:37:38.808364Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.80827Z",
     "shell.execute_reply": "2025-07-05T17:37:38.808278Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n\npos = mds.fit_transform(euclideanDistance)\n\nxs, ys = pos[:, 0], pos[:, 1]",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T06:13:07.814562400Z",
     "start_time": "2023-12-03T06:13:07.617656300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.809879Z",
     "iopub.status.idle": "2025-07-05T17:37:38.810202Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.810038Z",
     "shell.execute_reply": "2025-07-05T17:37:38.81005Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for x, y, name in zip(\n",
    "    xs, ys, list(unique_word_dict1.keys()) + list(unique_word_dict2.keys())\n",
    "):\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(name, (x, y))\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T06:13:18.743773100Z",
     "start_time": "2023-12-03T06:13:12.205656700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "editable": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T17:37:38.810929Z",
     "iopub.status.idle": "2025-07-05T17:37:38.811226Z",
     "shell.execute_reply.started": "2025-07-05T17:37:38.81107Z",
     "shell.execute_reply": "2025-07-05T17:37:38.811083Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}